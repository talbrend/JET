{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "Learn to use visualization techniques to study:\n",
    "1. missing data\n",
    "2. distributions\n",
    "3. correlation heatmaps\n",
    "4. pairplots,\n",
    "5. t-SNE\n",
    "\n",
    "pre-proc:\n",
    "1. use catboost for categoric data\n",
    "\n",
    "model: \n",
    "\n",
    "# tal todo\n",
    "1. Anamaly detection algo, price, or Best using the pipe class\n",
    "2. check anomaly+ catboost\n",
    "3. check pipes + transform all\n",
    "4. predict with stacking\n",
    "5. hyper paramentr tuning\n",
    "# efrat todo\n",
    "5. one-hot to categoric\n",
    "6. split to different models for low and high prices\n",
    "7. check reressions \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5v5mm4amQRrm",
    "papermill": {
     "duration": 0.010092,
     "end_time": "2023-03-07T06:21:39.774967",
     "exception": false,
     "start_time": "2023-03-07T06:21:39.764875",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# JET House Prices Prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# instruction\n",
    "## Part 1:\n",
    "[Model submission is done through Kaggle]\n",
    "Part 1 - EDA\n",
    "1. Which 3 features have the highest number of missing values\n",
    "2. How the price behave over the years?\n",
    "3. Plot the the feature distribution using histograms\n",
    "4. Compute and order the features by their correlation with label\n",
    "5. Add more EDA that will help you understand the data and support your modeling decisions\n",
    "\n",
    "Part 2 - baseline\n",
    "1. Train the simplest baseline model possible\n",
    "2. submit your baseline results and share the results\n",
    "\n",
    "## Part 2:\n",
    "[Model submission is done through Kaggle]\n",
    "Your solution:  go wild and build the best performing model!Which model you choose and why (relate to relevant EDA)?\n",
    "Choose a validation creation process, why you choose it? what is the baseline performance?\n",
    "Which smart tricks you used to boost your model performance?\n",
    "Describe potential generalization issues (e.g. overfit/underfit)? How can you handle these?\n",
    "submit your model and improve your rank in the leaderboard!\n",
    "\n",
    "Deliverables\n",
    "A final submission and score of you team in the leaderboard\n",
    "Working and easy to follow notebook with the 3 parts completed and that produce your best submission.\n",
    "Slides explaining your project: \n",
    "1. EDA\n",
    "2. decision made\n",
    "3. feature analysis\n",
    "4. validation\n",
    "5. modeling\n",
    "6. etc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVOXAyXl3-fA",
    "papermill": {
     "duration": 0.008317,
     "end_time": "2023-03-07T06:21:39.809564",
     "exception": false,
     "start_time": "2023-03-07T06:21:39.801247",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports and loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IGmyjJJatzBZ",
    "papermill": {
     "duration": 8.300496,
     "end_time": "2023-03-07T06:21:48.118668",
     "exception": false,
     "start_time": "2023-03-07T06:21:39.818172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#import tensorflow_decision_forests as tfdf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostRegressor\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import RidgeCV, LassoCV, ElasticNetCV\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Comment this if the data visualisations doesn't work on your side\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# general functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printt(df,Name=None,verbose=True):    \n",
    "    if verbose==True:\n",
    "        print('----------------------')\n",
    "        if Name!=None:\n",
    "            print(Name)\n",
    "        if isinstance(df, dict):\n",
    "            for key, value in df.items():\n",
    "                print(f\"{key}: {value}\")\n",
    "        else:\n",
    "            print(df)\n",
    "               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred(y, y_pred,verbose=True,addline = True):\n",
    "    if verbose==True:\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.scatter(y, y_pred, color='blue')\n",
    "        plt.title('True vs Predicted Values')\n",
    "        plt.xlabel('True Values')\n",
    "        plt.ylabel('Predicted Values')\n",
    "        if addline: plt.plot([min(y), max(y)], [min(y), max(y)], color='red') # Line for perfect predictions\n",
    "        plt.grid(True)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_path = \"project/test.csv\"\n",
    "train_file_path = \"project/train.csv\"\n",
    "testset_df = pd.read_csv(test_file_path)\n",
    "dataset_df = pd.read_csv(train_file_path)\n",
    "print(\"Full test set shape is {}\".format(testset_df.shape))\n",
    "print(\"Full train dataset shape is {}\".format(dataset_df.shape))\n",
    "id_file = testset_df['Id']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## log(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_df['SalePrice'] = np.log(dataset_df['SalePrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    dataset_df['MSSubClass'] = dataset_df['MSSubClass'].apply(str)\n",
    "    dataset_df['MoSold'] = dataset_df['MoSold'].astype(str)\n",
    "    testset_df['MSSubClass'] = testset_df['MSSubClass'].apply(str)\n",
    "    testset_df['MoSold'] = testset_df['MoSold'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_features_df = dataset_df.select_dtypes(include='number')\n",
    "numeric_features_df['LotFrontage'] = dataset_df.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n",
    "for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n",
    "    numeric_features_df[col] = numeric_features_df[col].fillna(0)\n",
    "numeric_features_df = numeric_features_df.fillna(numeric_features_df.mean())\n",
    "scaler = StandardScaler()\n",
    "standardized_data = pd.DataFrame(scaler.fit_transform(numeric_features_df), columns=numeric_features_df.columns, index=numeric_features_df.index)\n",
    "isloation_clf = IsolationForest(random_state=0).fit(standardized_data)\n",
    "isloation_clf.predict(standardized_data)\n",
    "iso_scores = isloation_clf.score_samples(standardized_data)\n",
    "iso_scores = pd.DataFrame({'score': iso_scores}, index=standardized_data.index)\n",
    "combined_df = pd.concat([iso_scores, standardized_data], axis=1).sort_values('score')\n",
    "combined_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import OneClassSVM\n",
    "svm_clf = OneClassSVM(nu=0.01).fit(standardized_data)\n",
    "svm_scores = svm_clf.score_samples(standardized_data)\n",
    "svm_scores = pd.DataFrame({'score': svm_scores}, index=standardized_data.index)\n",
    "combined_df = pd.concat([svm_scores, standardized_data], axis=1).sort_values('score')\n",
    "combined_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "scl = StandardScaler(with_mean=True,with_std=True)\n",
    "pca.fit(scl.fit_transform(standardized_data))\n",
    "\n",
    "def reduce_dim_reconstruct(data):\n",
    "  #Reduce the dimension, reconstruct. Standrtize data.\n",
    "  return scl.inverse_transform(pca.inverse_transform(pca.transform(scl.transform(data))))\n",
    "\n",
    "pca_scores = -1* ((standardized_data - reduce_dim_reconstruct(standardized_data)) ** 2).sum(axis=1)\n",
    "pca_scores = pd.DataFrame({'score': pca_scores}, index=standardized_data.index)\n",
    "combined_df = pd.concat([pca_scores, standardized_data], axis=1).sort_values('score')\n",
    "combined_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_scores = pd.DataFrame({'score': (svm_scores.score.rank()+iso_scores.score.rank() + pca_scores.score.rank())/3},index=standardized_data.index).sort_values('score')\n",
    "\n",
    "combined_df = pd.concat([ensemble_scores, standardized_data], axis=1).sort_values('score')\n",
    "combined_df.head(20)\n",
    "combined_df['score'].values[:15]\n",
    "combined_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PerformAnomalyDrop = False\n",
    "if PerformAnomalyDrop == True:\n",
    "    indices_to_show = [523, 1298, 30, 88, 462, 632, 1324]\n",
    "    entries_to_show = dataset_df.loc[indices_to_show]\n",
    "    indices_to_throw = [1298, 1182, 1386, 691, 185, 496 ]\n",
    "    #indices_to_throw = combined_df.index.values[:6]\n",
    "    #dataset_df = dataset_df[dataset_df.GrLivArea < 4500]\n",
    "    dataset_df =  dataset_df.drop(indices_to_throw)\n",
    "    dataset_df.reset_index(drop=True, inplace=True)\n",
    "    y = dataset_df['SalePrice'].reset_index(drop=True)\n",
    "    id_file = testset_df['Id']\n",
    "    entries_to_show[['MiscVal', 'SalePrice']]\n",
    "    #entries_to_show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_quantiles = combined_df['score'].quantile([0.01, 0.1, 0.25, 0.5, 0.75, 0.9])\n",
    "# custom_quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming 'dataset_df' is your DataFrame\n",
    "y = dataset_df['SalePrice']\n",
    "X = dataset_df.drop('SalePrice', axis=1)\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if 0:\n",
    "    # Assuming numerical_features is a list of column names of numerical features\n",
    "    numerical_features = dataset_df.select_dtypes(include=['int64', 'float64']).columns\n",
    "    \n",
    "    # Create box plots for all numerical features\n",
    "    # for feature in numerical_features:\n",
    "    #     plt.figure(figsize=(8, 6))\n",
    "    #     sns.boxplot(x=dataset_df[feature])\n",
    "    #     plt.title(f'Box plot of {feature}')\n",
    "    #     plt.show()\n",
    "    # Create box plots and scatter plots for each numerical feature\n",
    "    for feature in numerical_features:\n",
    "        # Create a new figure with two subplots in one row\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "    \n",
    "        # Box plot\n",
    "        sns.boxplot(x=dataset_df[feature], ax=axes[0])\n",
    "        axes[0].set_title(f'Box plot of {feature}')\n",
    "    \n",
    "        # Scatter plot\n",
    "        sns.scatterplot(x=dataset_df[feature], y=dataset_df['SalePrice'], ax=axes[1])\n",
    "        axes[1].set_title(f'Scatter plot of {feature} vs SalePrice')\n",
    "        axes[1].set_xlabel(feature)\n",
    "        axes[1].set_ylabel('SalePrice')\n",
    "    \n",
    "        # Adjust layout\n",
    "        plt.tight_layout()\n",
    "    \n",
    "        # Show the plots\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTnx8h9i416m",
    "papermill": {
     "duration": 0.008651,
     "end_time": "2023-03-07T06:21:48.263024",
     "exception": false,
     "start_time": "2023-03-07T06:21:48.254373",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The data is composed of 81 columns and 1460 entries. We can see all 81 dimensions of our dataset by printing out the first 3 entries using the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## explain_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_feature(feature_name, file_path=\"project/data_description.txt\"):\n",
    "    feature_name_lower = feature_name.lower()\n",
    "    with open(file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        collecting = False\n",
    "        for line in lines:\n",
    "            if feature_name_lower in line.lower():\n",
    "                collecting = True\n",
    "            elif collecting and \":\" in line and not line.lower().startswith(feature_name_lower):\n",
    "                # If we're collecting and encounter a line with a colon that doesn't start with the feature name,\n",
    "                # it's likely the start of another feature's description.\n",
    "                break\n",
    "            \n",
    "            if collecting:\n",
    "                print(line.strip())\n",
    "\n",
    "# Example usage\n",
    "explain_feature(\"MSSubClass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## categoric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_order_categoric_to_numeric(X,y=None,verbose=False):\n",
    "# Assuming X is your DataFrame\n",
    " if 0:\n",
    "    category_sets = [\n",
    "        set(['Ex', 'Gd', 'TA', 'Fa', 'Po', 'NotExist']),\n",
    "        set(['Gd', 'Av', 'Mn', 'No', 'NotExist']),\n",
    "        set(['GLQ','ALQ','BLQ','Rec','LwQ','Unf','NotExist']),\n",
    "        set(['Fin','RFn','Unf','NotExist']),\n",
    "        set(['GdPrv','MnPrv','GdWo','MnWw','NotExist']),\n",
    "    ]\n",
    "    titles = ['quality_mapping', 'access_mapping', 'basement_mapping', 'garage_mapping', 'fence_mapping']\n",
    "    \n",
    "    # Mappings\n",
    "    quality_mapping = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NotExist': 0}\n",
    "    access_mapping = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NotExist': 0}\n",
    "    basement_mapping = {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NotExist': 0}\n",
    "    garage_mapping = {'Fin': 3, 'RFn': 2, 'Unf': 1, 'NA': 0}\n",
    "    fence_mapping = {'GdPrv': 4, 'MnPrv': 3, 'GdWo': 2, 'MnWw': 1, 'NotExist': 0}\n",
    "    \n",
    "    # Mapping from titles to actual mappings\n",
    "    mapping_dict = {\n",
    "        'quality_mapping': quality_mapping,\n",
    "        'access_mapping': access_mapping,\n",
    "        'basement_mapping': basement_mapping,\n",
    "        'garage_mapping': garage_mapping,\n",
    "        'fence_mapping': fence_mapping,\n",
    "    }\n",
    "    \n",
    "    # Select columns of object type\n",
    "    col_object = X.select_dtypes(include=['O']).columns.tolist()\n",
    "    \n",
    "    matching_categories = {}\n",
    "    for col in col_object:\n",
    "        vals = set(X[col].unique())\n",
    "        # Check which category set vals belongs to\n",
    "        for index, category_set in enumerate(category_sets):\n",
    "            if vals.issubset(category_set):  # Check if all elements of vals are in the category_set\n",
    "                matching_categories[col] = titles[index]                \n",
    "                break  # Exit the loop if a matching set is found\n",
    "\n",
    "    # Apply the matched mappings to the columns\n",
    "    X_ = X.copy()\n",
    "    for col, title in matching_categories.items():\n",
    "        if title in mapping_dict:\n",
    "            X_[col] = X[col].map(mapping_dict[title])\n",
    "\n",
    "    if verbose==True:\n",
    "        printt(len(matching_categories),'matching_categories')\n",
    "        print(list(matching_categories.keys()))\n",
    "        for col, title in matching_categories.items():\n",
    "            if y is not None and not y.empty:\n",
    "                categoric_analysis(X[[col]],y,True)\n",
    "            print(f\"{col}: {title} : {mapping_dict[title]}\")\n",
    "    \n",
    "    return X_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoric_analysis(X,y,verbose=False):\n",
    "    #col_object = X.select_dtypes(include=['O']).columns.tolist()\n",
    "    cols = X.columns.tolist()\n",
    "    #cols = X.columns.tolist()\n",
    "    printt(len(cols),'number of categorical',verbose=verbose)\n",
    "    printt(cols,'categorical List',verbose=verbose)\n",
    "    i=0\n",
    "    for col in cols:\n",
    "        i = i+1\n",
    "        vals = X[col].unique()        \n",
    "        cols = [col for _ in range(len(vals))]  # This will create a list with 'title' repeated\n",
    "        #print('\\n'.join(str(val) for val in vals))\n",
    "        ymean  = [np.mean(y[X[col] == val])  for val in vals]\n",
    "        ystd   = [np.std(y[X[col] == val])   for val in vals]\n",
    "        ratios = [np.mean(X[col] == val)*100         for val in vals]\n",
    "        numbers = [np.sum(X[col] == val)             for val in vals]\n",
    "\n",
    "        df = pd.DataFrame({'col': cols, 'val': vals, 'ymean': ymean, 'ystd': ystd,'number': numbers, 'ratio': ratios})\n",
    "        df.loc[pd.isna(vals), 'ratio'] = np.mean(X[col].isna())*100\n",
    "        df.loc[pd.isna(vals), 'number'] = np.sum(X[col].isna())\n",
    "        df.loc[pd.isna(vals), 'ymean'] = np.mean(y[X[col].isna()])\n",
    "        df.loc[pd.isna(vals), 'ystd'] = np.std(y[X[col].isna()])\n",
    "        df = df.sort_values(by='ymean', ascending=True).reset_index(drop=True)  # Use ascending=False for descending order\n",
    "        # This will apply the formatting and then convert the DataFrame to a string for printing\n",
    "        formatted_df_string = df.to_string(formatters={'ymean': \"{:.2f}\".format, 'ystd': \"{:.2f}\".format, 'ratio': \"{:.2f}\".format})\n",
    "        printt('',col,verbose=verbose)      \n",
    "        if verbose==True:        \n",
    "            print(i)\n",
    "            explain_feature(col)\n",
    "            print(formatted_df_string)        \n",
    "            plot_categoric(df,col)\n",
    "    return df\n",
    "\n",
    "def plot_categoric(df,col):\n",
    "        ymean = df['ymean'].values\n",
    "        ystd = df['ystd'].values\n",
    "        numbers = df['number'].values\n",
    "        vals = df['val'].values        \n",
    "            # Plotting\n",
    "        plt.figure(figsize=(2, 2))\n",
    "        colors = plt.cm.rainbow(np.linspace(0, 1, len(vals)))  # Generate distinct colors\n",
    "\n",
    "        for i, mean in enumerate(ymean):\n",
    "            # Generate x values\n",
    "            x = np.linspace(norm.ppf(0.01, loc=mean, scale=ystd[i]),\n",
    "                            norm.ppf(0.99, loc=mean, scale=ystd[i]), 100)\n",
    "            # Generate y values for Gaussian curve\n",
    "            y_gauss = norm.pdf(x, loc=mean, scale=ystd[i]) * numbers[i]  # Scale by 'num'\n",
    "            plt.plot(x, y_gauss, label=f'{vals[i]} (n={numbers[i]})', color=colors[i])\n",
    "\n",
    "        plt.title(f'{col}')\n",
    "        plt.legend()\n",
    "        plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "        plt.subplots_adjust(right=0.75)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "#categoric_analysis(X[['ExterQual']],y,True)\n",
    "#categoric_analysis(X[['Condition1']],y,True)\n",
    " \n",
    "manual_feature_importance = {\n",
    "    'not': ['Street','LandContour','Utilities','LotConfig','LandSlope','Condition2','RoofMatl','ExterCond','Heating','Functional','GarageQual','GarageCond','PavedDrive','PoolQC','Fence','MiscFeature'],\n",
    "    'low': ['Alley','Condition1','BsmtCond','BsmtFinType2','CentralAir','Electrical'],\n",
    "    'med': ['MSZoning','BldgType','HouseStyle','RoofStyle','Exterior1st','Exterior2nd','BsmtExposure','BsmtFinType1','SaleType','SaleCondition'],\n",
    "    'high': ['LotShape','Neighborhood','MasVnrType','ExterQual','Foundation','BsmtQual','HeatingQC','KitchenQual','FireplaceQu','GarageType','GarageFinish']\n",
    "}\n",
    "#example of use\n",
    "if 0:\n",
    "    categoric_analysis(X[manual_feature_importance['high']],y,True)\n",
    "if 1:\n",
    "    df = categoric_analysis(X[['BldgType']],y,True)\n",
    "if 0:\n",
    "    df = categoric_analysis(X[['MoSold','MSSubClass']],y,True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoric_to_numeric_maping(df,MinRatioPerGroup = 10 ,verbose=False):\n",
    "    #df = pd.DataFrame({'col': cols, 'val': vals, 'ymean': ymean, 'ystd': ystd,'number': numbers, 'ratio': ratios})\n",
    "    MinRatioPerGroup = 100/np.floor(100/MinRatioPerGroup)\n",
    "    # diveide to groups\n",
    "    sum_ratio = 0\n",
    "    group = 0\n",
    "    for i in range(len(df)):\n",
    "        sum_ratio = sum_ratio + df.loc[i,'ratio']        \n",
    "        df.loc[i,'group'] = group\n",
    "        if sum_ratio>=MinRatioPerGroup:\n",
    "           sum_ratio = 0\n",
    "           group = group + 1        \n",
    "    # add last group to prev if it's too small\n",
    "    last_group_indexes = (df['group'] == group)\n",
    "    if np.sum(df.loc[last_group_indexes,'ratio'])<MinRatioPerGroup:\n",
    "        df.loc[last_group_indexes,'group'] = group-1\n",
    "\n",
    "    # add last group to prev if it's too small\n",
    "    if np.sum(df.loc[last_group_indexes,'ratio'])<MinRatioPerGroup:        \n",
    "        df.loc[last_group_indexes,'group'] = group-1\n",
    "\n",
    "    NGroups = int(df['group'].max()) + 1\n",
    "    # calc mean y per group\n",
    "    for g in range(NGroups):\n",
    "        group_indexes = (df['group'] == g)\n",
    "        ymean_group = np.sum(df.loc[group_indexes,'ymean']*df.loc[group_indexes,'number'])/np.sum(df.loc[group_indexes,'number'])\n",
    "        df.loc[group_indexes,'ymean_group'] = ymean_group\n",
    "        df.loc[group_indexes,'number_group'] = np.sum(df.loc[group_indexes,'number'])\n",
    "        df.loc[group_indexes,'ratio_group'] = np.sum(df.loc[group_indexes,'ratio'])\n",
    "    #print(df)\n",
    "    val_to_ymean_map = df.set_index('val')['ymean_group'].to_dict()\n",
    "    val_to_group_map = df.set_index('val')['group'].to_dict()\n",
    "    return val_to_ymean_map,val_to_group_map\n",
    "\n",
    "\n",
    "def categoric_to_numeric_fit(X,y,categortic_Config_params,cols=None, JustCategoric=True,verbose=False):\n",
    "    MinRatioPerGroup = categortic_Config_params['MinRatioPerGroup']\n",
    "    if JustCategoric==True:\n",
    "        exist_cols = X.select_dtypes(include=['O']).columns.tolist()\n",
    "    else:\n",
    "        exist_cols = X.columns.tolist()\n",
    "\n",
    "    if cols is None:\n",
    "        cols = exist_cols\n",
    "    else:\n",
    "        filtered_cols = [col for col in cols if col in exist_cols]\n",
    "        cols = filtered_cols\n",
    "    categoric_map = dict()\n",
    "    for col in cols:\n",
    "        df = categoric_analysis(X[[col]],y,False)\n",
    "        val_to_ymean_map,val_togroup_map = categoric_to_numeric_maping(df,MinRatioPerGroup,verbose=verbose)   \n",
    "        if categortic_Config_params['replaceby']=='ymean':\n",
    "            categoric_map[col] = val_to_ymean_map\n",
    "        else:\n",
    "            categoric_map[col] = val_togroup_map  \n",
    "        printt(df,'df in categoric_to_numeric_fit',verbose=verbose)\n",
    "        printt(categoric_map,'categoric_map in categoric_to_numeric_fit',verbose=verbose)\n",
    "    return categoric_map\n",
    "def categoric_to_numeric_transform(X,categoric_map,verbose=False):\n",
    "    X_ = X.copy()\n",
    "    for col in categoric_map:\n",
    "        X_[col] = X_[col].map(categoric_map[col])        \n",
    "    return X_\n",
    "\n",
    "#ExampleOfuse\n",
    "if 1:\n",
    "    categortic_Config_params1 = dict()\n",
    "    categortic_Config_params1['MinRatioPerGroup'] = 15\n",
    "    categortic_Config_params1['replaceby'] = 'ymean' #'ymean', 'group_nember'\n",
    "    cols = manual_feature_importance['high']\n",
    "    categoric_map = categoric_to_numeric_fit(X,y,categortic_Config_params1,cols=cols,JustCategoric=True,verbose=False)\n",
    "    X_numeric = categoric_to_numeric_transform(X,categoric_map,verbose=False)\n",
    "    \n",
    "    col_to_map_num = ['MSSubClass']\n",
    "    categoric_map_num = categoric_to_numeric_fit(X_numeric,y,categortic_Config_params1,cols=col_to_map_num,JustCategoric=False,verbose=False)\n",
    "    print(categoric_map_num)\n",
    "    X_numeric = categoric_to_numeric_transform(X_numeric,categoric_map_num,verbose=False)\n",
    "    \n",
    "    col_object1 = X.select_dtypes(include=['O']).columns.tolist()\n",
    "    col_object2 = X_numeric.select_dtypes(include=['O']).columns.tolist()\n",
    "    print(len(col_object1),len(col_object2))\n",
    "    printt(X_numeric['MSSubClass'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre proccessing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer to remove NAs\n",
    "class drop_ID(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, verbose=False):        \n",
    "        self.verbose = verbose\n",
    "    def fit(self, X, y=None):\n",
    "        printt('','In drop_ID fit',verbose=self.verbose)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        printt(X.shape,'drop_ID in X.shape',verbose=self.verbose)\n",
    "        # Ensure 'Id' column is removed safely\n",
    "        if 'Id' in X.columns:\n",
    "            return X.drop('Id', axis=1)\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imput_mean(X,mean):\n",
    "    for col in X.columns:        \n",
    "        X[col].fillna(mean, inplace=True)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imput_mean_mode(X,mean_mode):\n",
    "    # impute missing value by pre calc mean or mode :\n",
    "    for col in X.columns:        \n",
    "        X[col].fillna(mean_mode[col], inplace=True)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imput_mean_per_Neighborhood(X,mean_per_Neighborhood,mean_mode,feature='Neighborhood'):\n",
    "    col_numeric = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    for col in col_numeric:\n",
    "        row_index = pd.isna(X[col])\n",
    "        # Iterate over each row that contains NaN in the current column\n",
    "        for idx in X[row_index].index:\n",
    "            # Get the Neighborhood value for the current row\n",
    "            NeighborhoodVal = X.loc[idx, feature]            \n",
    "            # If the combination exists in the mean_per_Neighborhood, fill NaN with the mean value\n",
    "            if (NeighborhoodVal, col) in mean_per_Neighborhood:\n",
    "                X.loc[idx, col] = mean_per_Neighborhood[NeighborhoodVal, col]\n",
    "            else:\n",
    "                X.loc[idx, col] = mean_mode[col]\n",
    "    return X\n",
    "\n",
    "def calc_mean_Pef_feature(X,feature='Neighborhood'):\n",
    "    col_numeric = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    mean_per_Neighborhood = X.groupby(feature)[col_numeric].mean()\n",
    "    return mean_per_Neighborhood\n",
    "\n",
    "if 0:\n",
    "    mean_per_Neighborhood = calc_mean_Pef_feature(X,feature='Neighborhood')\n",
    "    X_New_ = imput_mean_per_Neighborhood(X,mean_per_Neighborhood,feature='Neighborhood')\n",
    "    printt(mean_per_Neighborhood,'combined_stats_perNeighborhood')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    \n",
    "def calc_mean_mode(X):\n",
    "    # find the mean of numrical columns and the mode of object columns\n",
    "    # return :\n",
    "    # combined_stats = mean amd mode in the same shape as orign,\n",
    "    # col_numeric,col_object =list of numeric and obkect coumln names\n",
    "    col_numeric = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    col_object = X.select_dtypes(include=['O']).columns.tolist()\n",
    "    printt(col_numeric,'col_numeric',False)\n",
    "    printt(col_object,'col_object',False)\n",
    "    # save the mean and the mode for later imputation\n",
    "    means = X[col_numeric].mean()\n",
    "    modes = X[col_object].mode().iloc[0]\n",
    "    # Step 3: Combine results, maintaining the original arrangement\n",
    "    combined_stats = pd.Series(dtype=object)\n",
    "    for col in X.columns:\n",
    "        if col in means:\n",
    "            combined_stats[col] = means[col]\n",
    "        elif col in modes:\n",
    "            combined_stats[col] = modes[col]\n",
    "    return combined_stats,col_numeric,col_object\n",
    "    \n",
    "if 0:\n",
    "    combined_stats = calc_mean_mode(X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bad_col(X,threshold=None,verbose=False):\n",
    "    # drop col with na in amount more than threshold\n",
    "    if threshold is not None:\n",
    "        thresh = len(X)*threshold//1        \n",
    "        na_counts    = X.isna().sum()\n",
    "        columns_to_keep_ = na_counts[na_counts < thresh].index\n",
    "        columns_to_remove_ = na_counts[na_counts >= thresh].index\n",
    "    else:\n",
    "        columns_to_keep_ = X.columns\n",
    "        columns_to_remove_ = pd.Index([])        \n",
    "\n",
    "    if verbose==True:\n",
    "        nan_counts  = X.isna().sum().sort_values(ascending=False).head(20)/len(X)*100\n",
    "        print(f\"features with the highest number of missing values in %\")\n",
    "        print(f\"{nan_counts}%\")\n",
    "        printt(len(columns_to_keep_),'len(columns_to_keep_)',verbose=verbose)\n",
    "        printt(len(columns_to_remove_),'len(columns_to_remove_)',verbose=verbose)\n",
    "        if len(columns_to_remove_)>0:\n",
    "            printt(columns_to_remove_,'columns_to_remove_',verbose=verbose)           \n",
    "\n",
    "    return columns_to_keep_,columns_to_remove_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_row_nan_counts(X,verbose=False):\n",
    "    # print number of Rows with missing values\n",
    "    row_nan_counts = X.isna().sum(axis=1) \n",
    "    top_row_nan_counts = row_nan_counts.sort_values(ascending=False)\n",
    "    if verbose==True:\n",
    "        print(\"number of Rows with missing values:\")\n",
    "        print(top_row_nan_counts.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_missingVal_groups(X,verbose=False):\n",
    "    row_nan_counts = X.isna().sum(axis=1) \n",
    "    rows_with_missing_values = X[row_nan_counts >0]\n",
    "    unique_missing_column_groups = set()\n",
    "    # Iterate through rows and identify unique groups of missing columns\n",
    "    for _, row in rows_with_missing_values.iterrows():\n",
    "        # Extract groups of 5 columns\n",
    "        groups = tuple(row.index[row.isna()])\n",
    "        # Add the unique group to the set\n",
    "        unique_missing_column_groups.add(groups)\n",
    "    \n",
    "    # Sort the unique groups by their length\n",
    "    sorted_unique_missing_column_groups = sorted(unique_missing_column_groups, key=lambda x: len(x))\n",
    "    # Print the unique groups of 5 missing columns\n",
    "    if verbose==True:\n",
    "        print('\\nmissing values groups:')\n",
    "        for i, group in enumerate(sorted_unique_missing_column_groups, start=1):\n",
    "            print(f\"Group {i} of {len(group)} missing columns: {group}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class remove_NAs_col(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=None,verbose=False):        \n",
    "        self.threshold = threshold  # Minimum non-NA values required to keep a column\n",
    "        self.verbose = verbose\n",
    "    def fit(self, X, y=None):\n",
    "        printt('','In RemoveNAs Col fit',verbose=self.verbose)\n",
    "        # If threshold is set, identify columns to keep based on the threshold\n",
    "        self.columns_to_keep_,self.columns_to_remove_ = find_bad_col(X,self.threshold,self.verbose)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        printt('','In RemoveNAs Col transform',verbose=self.verbose)\n",
    "        # Drop columns not meeting the threshold requirement\n",
    "        printt(X.shape,'X.shape',verbose=self.verbose)\n",
    "        X_transformed = X.loc[:, self.columns_to_keep_]\n",
    "        printt(X_transformed.shape,'X_transformed.shape',verbose=self.verbose)        \n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class imput_NAs_row(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, verbose=False):        \n",
    "        self.column_means_ = None\n",
    "        self.verbose = verbose\n",
    "        self.col_numeric = None\n",
    "        self.col_object = None\n",
    "        self.mean_per_Neighborhood = None\n",
    "    def fit(self, X, y=None):\n",
    "        printt('','In imput_NAs_row fit',verbose=self.verbose)\n",
    "        # Calculate mean values for each column, ignoring NA's\n",
    "        printt(X.shape,'imput_NAs_row in X.shape',verbose=self.verbose)\n",
    "        # save the mean and the mode for later imputation\n",
    "        self.mean_mode,self.col_numeric,self.col_object = calc_mean_mode(X)\n",
    "        self.mean_per_Neighborhood = calc_mean_Pef_feature(X,feature='Neighborhood')\n",
    "        print_row_nan_counts(X,verbose=self.verbose)\n",
    "        print_missingVal_groups(X,verbose=self.verbose)        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        printt('','In imput_NAs_row transform',verbose=self.verbose)\n",
    "        printt(X.shape,'X.shape',verbose=self.verbose)\n",
    "        print_row_nan_counts(X,verbose=self.verbose)\n",
    "\n",
    "        printt('','filing Garage and Bsmt',verbose=self.verbose)\n",
    "        # Replace NaN values in the specified columns for rows where 'TotalBsmtSF' is equal to 0 with 'NotExist'\n",
    "        basement_list = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\n",
    "        Garage_list = ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']\n",
    "        X.loc[X['TotalBsmtSF'] == 0, basement_list] = X.loc[X['TotalBsmtSF'] == 0, basement_list].fillna('NotExist')\n",
    "        X.loc[X['GarageArea'] == 0, Garage_list] = X.loc[X['GarageArea'] == 0, Garage_list].fillna('NotExist')\n",
    "        print_row_nan_counts(X,verbose=self.verbose)\n",
    "        # special imputation\n",
    "        printt('','special imputation',verbose=self.verbose)\n",
    "        X['GarageYrBlt'].fillna(X['YearBuilt'], inplace=True)        \n",
    "        X['GarageArea'].fillna(0,inplace=True)\n",
    "        X['GarageCars'].fillna(0,inplace=True)\n",
    "        X['BsmtFinType2'].fillna('NotExist', inplace=True)\n",
    "        X['BsmtExposure'].fillna('NotExist', inplace=True)\n",
    "        print_row_nan_counts(X,verbose=self.verbose)\n",
    "\n",
    "        printt('','imput_mean_mode',verbose=self.verbose)\n",
    "\n",
    "        imput_mean_per_Neighborhood_flag = False\n",
    "        imput_mean_mode_flag = True         \n",
    "        imput_0_Notexist = False         \n",
    "\n",
    "        if imput_mean_per_Neighborhood_flag==True:\n",
    "            X = imput_mean_per_Neighborhood(X,self.mean_per_Neighborhood,self.mean_mode,feature='Neighborhood')\n",
    "        if imput_mean_mode_flag == True:          \n",
    "            X = imput_mean_mode(X,self.mean_mode)\n",
    "        if imput_0_Notexist == True:          \n",
    "            X[self.col_numeric] = X[self.col_numeric].fillna(0)\n",
    "            X[self.col_object] = X[self.col_object].fillna('NotExist')        \n",
    "        print_row_nan_counts(X,verbose=self.verbose)\n",
    "        #printt(self.col_object)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## handle categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_qual_categoric(X,y=None,verbose=False):\n",
    "# Assuming X is your DataFrame\n",
    "\n",
    "    category_sets = [\n",
    "        set(['Ex', 'Gd', 'TA', 'Fa', 'Po', 'NotExist']),\n",
    "        set(['Gd', 'Av', 'Mn', 'No', 'NotExist']),\n",
    "        set(['GLQ','ALQ','BLQ','Rec','LwQ','Unf','NotExist']),\n",
    "        set(['Fin','RFn','Unf','NotExist']),\n",
    "        set(['GdPrv','MnPrv','GdWo','MnWw','NotExist']),\n",
    "    ]\n",
    "    titles = ['quality_mapping', 'access_mapping', 'basement_mapping', 'garage_mapping', 'fence_mapping']\n",
    "    \n",
    "    # Mappings\n",
    "    quality_mapping = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NotExist': 0}\n",
    "    access_mapping = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NotExist': 0}\n",
    "    basement_mapping = {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NotExist': 0}\n",
    "    garage_mapping = {'Fin': 3, 'RFn': 2, 'Unf': 1, 'NA': 0}\n",
    "    fence_mapping = {'GdPrv': 4, 'MnPrv': 3, 'GdWo': 2, 'MnWw': 1, 'NotExist': 0}\n",
    "    \n",
    "    # Mapping from titles to actual mappings\n",
    "    mapping_dict = {\n",
    "        'quality_mapping': quality_mapping,\n",
    "        'access_mapping': access_mapping,\n",
    "        'basement_mapping': basement_mapping,\n",
    "        'garage_mapping': garage_mapping,\n",
    "        'fence_mapping': fence_mapping,\n",
    "    }\n",
    "    \n",
    "    # Select columns of object type\n",
    "    col_object = X.select_dtypes(include=['O']).columns.tolist()\n",
    "    \n",
    "    matching_categories = {}\n",
    "    for col in col_object:\n",
    "        vals = set(X[col].unique())\n",
    "        # Check which category set vals belongs to\n",
    "        for index, category_set in enumerate(category_sets):\n",
    "            if vals.issubset(category_set):  # Check if all elements of vals are in the category_set\n",
    "                matching_categories[col] = titles[index]                \n",
    "                break  # Exit the loop if a matching set is found\n",
    "\n",
    "    # Apply the matched mappings to the columns\n",
    "    X_ = X.copy()\n",
    "    for col, title in matching_categories.items():\n",
    "        if title in mapping_dict:\n",
    "            X_[col] = X[col].map(mapping_dict[title])\n",
    "\n",
    "    if verbose==True:\n",
    "        printt(len(matching_categories),'matching_categories')\n",
    "        print(list(matching_categories.keys()))\n",
    "        for col, title in matching_categories.items():\n",
    "            if y is not None and not y.empty:\n",
    "                categoric_analysis(X[[col]],y,True)\n",
    "            print(f\"{col}: {title} : {mapping_dict[title]}\")\n",
    "    \n",
    "    return X_\n",
    "\n",
    "#replace_qual_categoric()\n",
    "def print_features(X,y,col_list=None): \n",
    "    if col_list is None:\n",
    "        col_list = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual', 'GarageFinish', 'GarageQual', 'GarageCond']\n",
    "    columns_to_keep_,columns_to_remove_ = find_bad_col(X,.15)\n",
    "    Xnew = X.loc[:, columns_to_keep_]\n",
    "    print(Xnew.shape)\n",
    "    imp = imput_NAs_row()\n",
    "    imp.fit(Xnew)\n",
    "    Xnew = imp.transform(Xnew)\n",
    "    print(Xnew.shape)\n",
    "    X_ = replace_qual_categoric(Xnew[col_list],y,verbose=True)\n",
    "    Xnew.shape,X_.shape\n",
    "\n",
    "if 0:\n",
    "    print_features(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class handle_categoric(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,categortic_Config_params,verbose=False,col_to_map=None):        \n",
    "        self.col_numeric = None\n",
    "        self.verbose = verbose\n",
    "        self.col_object = None\n",
    "        self.col_to_map = col_to_map\n",
    "        self.col_to_map_num = ['MoSold','MSSubClass']\n",
    "        self.categoric_map_num = None\n",
    "        self.categortic_Config_params = categortic_Config_params\n",
    "        self.ymean = None\n",
    "    def fit(self, X, y=None):\n",
    "        printt('','In categoric fit',verbose=self.verbose)\n",
    "        self.col_numeric = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "        self.col_object = X.select_dtypes(include=['O']).columns.tolist()\n",
    "        #col_to_map = manual_feature_importance['high']\n",
    "        col_to_map = self.col_object\n",
    "        self.categoric_map  = categoric_to_numeric_fit(X,y,self.categortic_Config_params,cols=col_to_map,JustCategoric=True,verbose=self.verbose)\n",
    "        self.categoric_map_num = categoric_to_numeric_fit(X,y,self.categortic_Config_params,cols=self.col_to_map_num,JustCategoric=False,verbose=self.verbose)\n",
    "        self.ymean = np.mean(y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        printt(f\"drop_categoric: {self.categortic_Config_params['drop_categoric']}\",'',verbose=self.verbose)\n",
    "        # for now remove all categoric TODO: improve this\n",
    "        printt('','In categoric transform',verbose=self.verbose)\n",
    "        printt(X.shape,'X.shape',verbose=self.verbose)\n",
    "\n",
    "        if self.categortic_Config_params['drop_categoric']=='KeepJustOrdered' :\n",
    "            X = replace_qual_categoric(X,verbose=self.verbose)\n",
    "            col_numeric = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "            left_objects = len(X.select_dtypes(include=['O']).columns.tolist())\n",
    "            printt('',f'number of num feature befor {len(self.col_numeric)}, added {len(col_numeric) - len(self.col_numeric)}, left_objects: {left_objects}',verbose=self.verbose)\n",
    "            X = X.loc[:, col_numeric]\n",
    "\n",
    "        if self.categortic_Config_params['drop_categoric'] == 'TransformAll':       \n",
    "            printt('','I am in Transform Categoric',verbose=self.verbose)\n",
    "            X = categoric_to_numeric_transform(X,self.categoric_map,verbose=self.verbose)\n",
    "            X = categoric_to_numeric_transform(X,self.categoric_map_num,verbose=self.verbose)\n",
    "            X = imput_mean(X,self.ymean)\n",
    "            #display(X)\n",
    "            col_numeric = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "            X = X.loc[:, col_numeric]            \n",
    "            \n",
    "        if self.categortic_Config_params['drop_categoric'] =='DropAll':\n",
    "            col_numeric = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "            X = X.loc[:, col_numeric]\n",
    "\n",
    "        #printt(X.shape,'X.shape')\n",
    "        #printt(len(X.select_dtypes(include=['O']).columns.tolist()),'length Object')\n",
    "        #printt(X.select_dtypes(include=['O']).columns.tolist())\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeatureEngineering(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,PefrormFeatureEngineering=True,verbose=False):        \n",
    "        self.verbose = verbose\n",
    "        self.PefrormFeatureEngineering = PefrormFeatureEngineering\n",
    "    def fit(self, X, y=None):\n",
    "        return self    \n",
    "    def transform(self, X):\n",
    "        if self.PefrormFeatureEngineering==True:\n",
    "            printt(X.shape,'shape befor FeatureEngineering',self.verbose)\n",
    "            X['YrBltAndRemod']=X['YearBuilt']+X['YearRemodAdd']\n",
    "            X['TotalSF']=X['TotalBsmtSF'] + X['1stFlrSF'] + X['2ndFlrSF']\n",
    "            \n",
    "            X['Total_sqr_footage'] = (X['BsmtFinSF1'] + X['BsmtFinSF2'] +\n",
    "                                             X['1stFlrSF'] + X['2ndFlrSF'])\n",
    "            \n",
    "            X['Total_Bathrooms'] = (X['FullBath'] + (0.5 * X['HalfBath']) +\n",
    "                                           X['BsmtFullBath'] + (0.5 * X['BsmtHalfBath']))\n",
    "            \n",
    "            X['Total_porch_sf'] = (X['OpenPorchSF'] + X['3SsnPorch'] +\n",
    "                                          X['EnclosedPorch'] + X['ScreenPorch'] +\n",
    "                                          X['WoodDeckSF'])\n",
    "            X['haspool'] = X['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "            X['has2ndfloor'] = X['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "            X['hasgarage'] = X['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\n",
    "            X['hasbsmt'] = X['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\n",
    "            X['hasfireplace'] = X['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)\n",
    "            printt(X.shape,'shape After FeatureEngineering',self.verbose)            \n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Anomaly Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnomalyDetection(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,verbose=False):        \n",
    "        self.verbose = verbose\n",
    "    def fit(self, X, y=None):\n",
    "        return self    \n",
    "    def transform(self, X):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categortic_Config_params = dict()\n",
    "categortic_Config_params['MinRatioPerGroup'] = 2\n",
    "categortic_Config_params['replaceby'] = 'ymean' #'ymean', 'group_nember'\n",
    "#categortic_Config_params['categoric_to_replace'] ='just_high' # 'All','from_low_to_high','from_med_to_high','just_high'\n",
    "categortic_Config_params['drop_categoric'] = 'TransformAll' #'KeepAll','KeepJustOrdered','TransformAll','DropAll', KeepAll is the best option\n",
    "\n",
    "Config_params = dict()\n",
    "Config_params['PefrormFeatureEngineering'] = True\n",
    "Config_params['nan_counts_threshold'] = .15\n",
    "Config_params['filter_out_HighNA'] = 1\n",
    "Config_params[\"model\"]='CatBoost' #'DecisionTree','CatBoost', CatBoost is the best option\n",
    "Config_params[\"take_log_target\"] = 0 # 0,1  . 1 is the best option\n",
    "Config_params[\"verbose\"]=False # False,True\n",
    "Config_params[\"categortic_Config_params\"] = categortic_Config_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and return a pipeline based on the configuration\n",
    "def create_pipeline(Config_params):\n",
    "    verbose = Config_params[\"verbose\"]\n",
    "    categortic_Config_params = Config_params[\"categortic_Config_params\"]\n",
    "    printt(Config_params['model'],verbose = verbose)\n",
    "    # Dynamically select the model based on Config_params\n",
    "    printt(Config_params['model'],'Model',verbose=verbose)\n",
    "    if Config_params['model'] == 'LinearRegression':\n",
    "        model = LinearRegression()\n",
    "    if Config_params['model'] == 'DecisionTree':\n",
    "        model = DecisionTreeRegressor(random_state=42)\n",
    "    elif Config_params['model'] == 'CatBoost':\n",
    "        #\n",
    "        if categortic_Config_params['drop_categoric'] == 'KeepAll':\n",
    "            #categorical_columns = X.select_dtypes(include=['O']).columns.tolist()\n",
    "            categorical_columns = ['MSSubClass','YrSold','MSZoning', 'Street', 'LotShape', 'LandContour',\n",
    "                               'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood',\n",
    "                               'Condition1', 'Condition2', 'BldgType', 'HouseStyle',\n",
    "                               'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd',\n",
    "                               'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
    "                               'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n",
    "                               'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n",
    "                               'Functional', 'GarageType', 'GarageFinish', 'GarageQual',\n",
    "                               'GarageCond', 'PavedDrive', 'SaleType', 'SaleCondition']\n",
    "            model = CatBoostRegressor(random_seed=42, silent=True, cat_features = categorical_columns)\n",
    "        else:\n",
    "            model = CatBoostRegressor(random_state=42, verbose=int(verbose))  # Adjust verbosity for CatBoost\n",
    "        #printt(len(categorical_columns),'length categorical_columns')\n",
    "        #print(categorical_columns)\n",
    "    if (Config_params['model'] == 'CatBoost') & (categortic_Config_params['drop_categoric'] == 'KeepAll'):\n",
    "        pipeline = Pipeline([\n",
    "            ('drop_ID', drop_ID(verbose=verbose)),\n",
    "            ('remove_nas_col', remove_NAs_col(threshold=Config_params['nan_counts_threshold'], verbose=verbose)),\n",
    "            ('imput_nas_row', imput_NAs_row(verbose=verbose)),\n",
    "            ('FeatureEngineering',FeatureEngineering(PefrormFeatureEngineering=Config_params['PefrormFeatureEngineering'],verbose=verbose)),\n",
    "            ('handle_categoric', handle_categoric(categortic_Config_params,verbose=verbose)),\n",
    "            ('reg', model)  # Use the dynamically selected model\n",
    "        ])\n",
    "    else:\n",
    "        pipeline = Pipeline([\n",
    "            ('drop_ID', drop_ID(verbose=verbose)),\n",
    "            ('remove_nas_col', remove_NAs_col(threshold=Config_params['nan_counts_threshold'], verbose=verbose)),\n",
    "            ('imput_nas_row', imput_NAs_row(verbose=verbose)),\n",
    "            ('FeatureEngineering',FeatureEngineering(PefrormFeatureEngineering=Config_params['PefrormFeatureEngineering'],verbose=verbose)),\n",
    "            ('handle_categoric', handle_categoric(categortic_Config_params,verbose=verbose)),\n",
    "            ('robust_scaler', RobustScaler()),\n",
    "            ('reg', model)  # Use the dynamically selected model\n",
    "        ])\n",
    "\n",
    "    \n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross val model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kfolds = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "def cv_rmse(model, X,y):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n",
    "    return (rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def DecisionTreeCV(X,y,title,use_cv_rmse=False, x_t=None,y_t=None):\n",
    "    Config_params1 = Config_params\n",
    "    Config_params1[\"model\"] = 'DecisionTree'\n",
    "    pipeline1 = create_pipeline(Config_params1)\n",
    "    run_pipe(pipeline1, X, y,title,use_cv_rmse, x_t,y_t)\n",
    "\n",
    "def LinearModelCV(X,y,title,use_cv_rmse=False, x_t=None,y_t=None):\n",
    "    Config_params1 = Config_params\n",
    "    Config_params1[\"model\"] = 'LinearRegression'\n",
    "    pipeline1 = create_pipeline(Config_params1)\n",
    "    run_pipe(pipeline1, X, y,title,use_cv_rmse, x_t,y_t)\n",
    "\n",
    "def CatTransllModelCV(X,y,title,use_cv_rmse=False, x_t=None,y_t=None):\n",
    "    Config_params1 = Config_params\n",
    "    Config_params1[\"model\"] = 'CatBoost'\n",
    "    Config_params1[\"drop_categoric\"] = 'TransformAll'\n",
    "    categortic_Config_params[\"drop_categoric\"] = 'TransformAll'\n",
    "    Config_params1[\"categortic_Config_params\"] = categortic_Config_params\n",
    "    pipeline1 = create_pipeline(Config_params1)\n",
    "    run_pipe(pipeline1, X, y,title,use_cv_rmse, x_t,y_t)\n",
    "\n",
    "def CatKeepAllModelCV(X,y,title,use_cv_rmse=False, x_t=None,y_t=None):\n",
    "    Config_params1 = Config_params\n",
    "    Config_params1[\"model\"] = 'CatBoost'\n",
    "    Config_params1[\"drop_categoric\"] = 'KeepAll'\n",
    "    categortic_Config_params[\"drop_categoric\"] = 'KeepAll'\n",
    "    Config_params1[\"categortic_Config_params\"] = categortic_Config_params\n",
    "    pipeline1 = create_pipeline(Config_params1)\n",
    "    run_pipe(pipeline1, X, y,title,use_cv_rmse, x_t,y_t)\n",
    "\n",
    "def run_pipe(pipeline, X, y,title,use_cv_rmse=False, x_t=None,y_t=None):\n",
    "    if use_cv_rmse==True:\n",
    "        rms_log = cv_rmse(pipeline, X, y)\n",
    "        print(f\"LogRMS with CV {title} : {rms_log.mean():.4f} ({rms_log.std():.4f})\")\n",
    "    else:\n",
    "        pipeline.fit(X,y)\n",
    "        y_pred = pipeline.predict(x_t)        \n",
    "        rms_log = np.sqrt(mean_squared_error(y_t, y_pred))        \n",
    "        print(f\"logRMS {title} = {rms_log:.5f}\")\n",
    "\n",
    "use_cv_rmse=True\n",
    "if use_cv_rmse==False:\n",
    "    DecisionTreeCV(X_train,y_train,'DecisionTreeCV',use_cv_rmse,X_validation,y_validation)\n",
    "    LinearModelCV(X_train,y_train,'LinearModel',use_cv_rmse,X_validation,y_validation)\n",
    "    CatTransllModelCV(X_train,y_train,'CatTransll',use_cv_rmse,X_validation,y_validation)\n",
    "    CatKeepAllModelCV(X_train,y_train,'CatKeepAll',use_cv_rmse,X_validation,y_validation)\n",
    "else:\n",
    "    DecisionTreeCV(X,y,'DecisionTreeCV',use_cv_rmse)\n",
    "    LinearModelCV(X,y,'LinearModel',use_cv_rmse)\n",
    "    CatTransllModelCV(X,y,'CatTransll',use_cv_rmse)\n",
    "    CatKeepAllModelCV(X,y,'CatKeepAll',use_cv_rmse)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    model_lr = LinearRegression()\n",
    "    model_dt = DecisionTreeRegressor(random_state=42)\n",
    "    model_cat = CatBoostRegressor(random_state=42, verbose=0)  # Adjust verbosity for CatBoost\n",
    "    \n",
    "    pipeline = create_pipeline(Config_params)\n",
    "    XX = pipeline.named_steps['drop_ID'].fit_transform(X_train)\n",
    "    XX = pipeline.named_steps['remove_nas_col'].fit_transform(XX)\n",
    "    XX = pipeline.named_steps['imput_nas_row'].fit_transform(XX)\n",
    "    XX = pipeline.named_steps['handle_categoric'].fit_transform(XX,y=y_train)\n",
    "    XX = pipeline.named_steps['robust_scaler'].fit_transform(XX)\n",
    "    model_lr.fit(XX,y=y_train)\n",
    "    model_dt.fit(XX,y=y_train)\n",
    "    model_cat.fit(XX,y=y_train)\n",
    "    #pipeline.named_steps['reg'].fit(XX,y=y_train)\n",
    "    \n",
    "    \n",
    "    XX = pipeline.named_steps['drop_ID'].transform(X_validation)\n",
    "    XX = pipeline.named_steps['remove_nas_col'].transform(XX)\n",
    "    XX = pipeline.named_steps['imput_nas_row'].transform(XX)\n",
    "    XX = pipeline.named_steps['handle_categoric'].transform(XX)\n",
    "    XX = pipeline.named_steps['robust_scaler'].transform(XX)\n",
    "    #y_pred = pipeline.named_steps['reg'].predict(XX)\n",
    "    y_lr = model_lr.predict(XX)\n",
    "    y_dt = model_dt.predict(XX)\n",
    "    y_cat = model_cat.predict(XX)\n",
    "\n",
    "    def CatKeepAllModel(X,y,x_test):\n",
    "        Config_params1 = Config_params\n",
    "        Config_params1[\"model\"] = 'CatBoost'\n",
    "        Config_params1[\"drop_categoric\"] = 'KeepAll'\n",
    "        Config_params1[\"categortic_Config_params\"] = {**categortic_Config_params, \"drop_categoric\": 'KeepAll'}\n",
    "        pipeline = create_pipeline(Config_params1)\n",
    "        pipeline.fit(X,y)\n",
    "        y_cat_keepAll = pipeline.predict(x_test)\n",
    "        return y_cat_keepAll\n",
    "    \n",
    "    def CombModel(X,y,x_test):\n",
    "        Config_params1 = Config_params\n",
    "        Config_params1[\"model\"] = 'CatBoost'\n",
    "        pipeline = create_pipeline(Config_params)\n",
    "        pipeline.fit(X_train,y_train)\n",
    "        y_cat = pipeline.predict(x_test)\n",
    "        Config_params1[\"model\"] = 'DecisionTree'\n",
    "        pipeline = create_pipeline(Config_params)\n",
    "        pipeline.fit(X_train,y_train)\n",
    "        y_dt = pipeline.predict(x_test)  \n",
    "        y_comb = y_cat.copy()\n",
    "        y_comb[(y_dt<10.9) | (y_dt>13)] = y_dt[(y_dt<10.9) | (y_dt>13)]\n",
    "        return y_comb\n",
    "    \n",
    "    \n",
    "    rms_log = np.sqrt(mean_squared_error(y_validation, y_dt))        \n",
    "    print(f\"logRMS y_dt = {rms_log:.5f}\")\n",
    "    rms_log = np.sqrt(mean_squared_error(y_validation, y_lr))        \n",
    "    print(f\"logRMS y_lr = {rms_log:.5f}\")\n",
    "    rms_log = np.sqrt(mean_squared_error(y_validation, y_cat))        \n",
    "    print(f\"logRMS y_cat = {rms_log:.5f}\")\n",
    "    \n",
    "    y_comb = CombModel(X_train,y_train,X_validation)\n",
    "    rms_log = np.sqrt(mean_squared_error(y_validation, y_comb))        \n",
    "    print(f\"logRMS y_comb = {rms_log:.5f}\")\n",
    "    \n",
    "    y_cat_keepAll = CatKeepAllModel(X_train,y_train,X_validation)\n",
    "    rms_log = np.sqrt(mean_squared_error(y_validation, y_cat_keepAll))        \n",
    "    print(f\"logRMS y_cat_keepAll = {rms_log:.5f}\")\n",
    "    \n",
    "    plot_pred(y_validation, y_comb,True) \n",
    "    plot_pred(y_validation, y_cat,True) \n",
    "    plot_pred(y_validation, y_lr,True)     \n",
    "    plot_pred(y_validation, y_dt,True)     \n",
    "\n",
    "    \n",
    "#XX.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyper parameter:\n",
    "# check best MinRatioPerGroupV with CatBoost\n",
    "if 0:\n",
    "    MinRatioPerGroupV = [0.01,1,2,4,6,10,15,20,50] #'KeepAll','KeepJustOrdered','DropAll','TransformAll'\n",
    "    modelV = ['LinearRegression','CatBoost'],\n",
    "    for model in modelV:\n",
    "        for MinRatioPerGroup in MinRatioPerGroupV:\n",
    "            # update config\n",
    "            Config_params1 = Config_params\n",
    "            Config_params1['Model'] = model\n",
    "            categortic_Config_params['MinRatioPerGroup'] = MinRatioPerGroup\n",
    "            Config_params1[\"categortic_Config_params\"] = categortic_Config_params\n",
    "            pipeline1 = create_pipeline(Config_params1)\n",
    "            #pipeline1.fit(X_train,y_train)\n",
    "            #y_cat = pipeline1.predict(X_validation)\n",
    "            rms_log = cv_rmse(pipeline1, X, y)\n",
    "            print(f\"LogRMS with CV MinRatioPerGroup={MinRatioPerGroup}: {rms_log.mean():.4f} ({rms_log.std():.4f})\")\n",
    "            #print(\"LogRMS with CV MinRatioPerGroup={}: {:.4f} ({:.4f})\\n\".format(MinRatioPerGroup,rms_log.mean(), rms_log.std()))\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    TSNE_model = TSNE(n_components=2, random_state=0, perplexity=50)\n",
    "    List = ['OverallQual','GrLivArea','GarageCars','GarageArea','TotalBsmtSF','LotShape','Neighborhood','ExterQual','Foundation','BsmtQual','HeatingQC','KitchenQual','GarageType','GarageFinish']\n",
    "    #List = ['GrLivArea','GarageArea','TotalBsmtSF']\n",
    "    \n",
    "    tsne = TSNE_model.fit_transform(XX[List])\n",
    "    if 1:\n",
    "        pca = PCA(n_components=2)\n",
    "        pca.fit(XX[List])\n",
    "        XX_pca = pca.transform(XX[List])\n",
    "        kmeans = KMeans(n_clusters=5)\n",
    "        kmeans.fit(XX[List])\n",
    "        fr = pd.DataFrame({'tsne1': tsne[:,0], 'tsne2': tsne[:, 1], 'cluster': kmeans.labels_})\n",
    "        print(np.sum(pca.explained_variance_ratio_))\n",
    "        sns.lmplot(data=fr, x='tsne1', y='tsne2', hue='cluster', fit_reg=False)\n",
    "    \n",
    "    fr = pd.DataFrame({'tsne1': tsne[:,0], 'tsne2': tsne[:, 1], 'cluster': y_train<11.5})\n",
    "    sns.lmplot(data=fr, x='tsne1', y='tsne2', hue='cluster', fit_reg=False)\n",
    "    \n",
    "    unique_labels = np.unique(kmeans.labels_)\n",
    "    means = {label: y_train[kmeans.labels_ == label].mean() for label in unique_labels}\n",
    "    std = {label: y_train[kmeans.labels_ == label].std() for label in unique_labels}\n",
    "    for label in unique_labels:\n",
    "        print(f'Mean of cluster {label}: {means[label]}, {std[label]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all combinations\n",
    "models = ['LinearRegression','CatBoost'] #['DecisionTree', 'CatBoost']\n",
    "models = ['CatBoost'] #['DecisionTree', 'CatBoost']\n",
    "\n",
    "drop_categoric_options = ['TransformAll'] #'KeepAll','KeepJustOrdered','DropAll','TransformAll'\n",
    "\n",
    "i=0\n",
    "pipelines = {}\n",
    "for model in models:\n",
    "    for drop_categoric in drop_categoric_options:\n",
    "            # update config\n",
    "            Config_params1 = Config_params\n",
    "            Config_params1[\"model\"] = model\n",
    "            Config_params1[\"drop_categoric\"] = drop_categoric\n",
    "            Config_params1[\"categortic_Config_params\"] = {**categortic_Config_params, \"drop_categoric\": drop_categoric}\n",
    "\n",
    "            # create pipe\n",
    "            print(f\"Pipeline created for model: {model}, drop_categoric: {drop_categoric}\")\n",
    "            pipeline = create_pipeline(Config_params1)\n",
    "            print('Start fit')\n",
    "            pipeline.fit(X_train,y_train)\n",
    "            print('Start predict')\n",
    "            y_pred = pipeline.predict(X_validation)\n",
    "    \n",
    "            # check errors \n",
    "            mask = y_validation > 0\n",
    "            rms_log = np.sqrt(mean_squared_error(y_validation[mask], y_pred[mask]))        \n",
    "            print(f\"logRMS = {rms_log:.5f}\")\n",
    "            pipelines[i] = pipeline\n",
    "            i=i+1     \n",
    "            \n",
    "\n",
    "# create pipe\n",
    "rms_log = cv_rmse(pipeline, X, y)\n",
    "print(\"LogRMS with CV: {:.4f} ({:.4f})\\n\".format(rms_log.mean(), rms_log.std()))\n",
    "#plot_pred(y_validation, y_pred,True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_low_prices(y,threshold):\n",
    "    values = y[y <= threshold]\n",
    "    indexes = np.where(y <= threshold)[0]\n",
    "    #print(y.iloc[indexes])\n",
    "    #print(np.exp(values))\n",
    "    #print(index)\n",
    "    return indexes\n",
    "find_low_prices(y_validation,threshold=11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pipelines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Post_Proccessing(pipeline):\n",
    "    model = pipeline.named_steps['reg']\n",
    "    \n",
    "    # Step 3: Get Feature Importances\n",
    "    feature_importances = model.get_feature_importance()\n",
    "    feature_names = model.feature_names_\n",
    "    sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "    \n",
    "    # Limit to first 8 features\n",
    "    top_indices = sorted_indices[:8]\n",
    "    top_importances = feature_importances[top_indices]\n",
    "    top_names = np.array(feature_names)[top_indices]\n",
    "\n",
    "    printt(top_names,'top_names')\n",
    "    #print_features(X,y,top_names)\n",
    "    # Plot\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.title(\"Top 8 Feature Importances\")\n",
    "    plt.barh(range(len(top_importances)), top_importances, align=\"center\")\n",
    "    plt.yticks(range(len(top_importances)), top_names)\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to have the feature with the highest importance on top\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def Post_Proccessing_shap(pipeline):\n",
    "    model = pipeline.named_steps['reg']\n",
    "\n",
    "    # Calculate SHAP values\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_train)\n",
    "    \n",
    "    # Summarize the effects of all the features\n",
    "    shap.summary_plot(shap_values, X_train, plot_type=\"bar\")\n",
    "\n",
    "Post_Proccessing(pipelines[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if 0:\n",
    "    # Config_params[\"model\"] = model\n",
    "    # Config_params[\"drop_categoric\"] = drop_categoric\n",
    "    drop_categoric = \"TransformAll\"\n",
    "    Config_params[\"categortic_Config_params\"] = {**categortic_Config_params, \"drop_categoric\": drop_categoric}\n",
    "    pipeline = create_pipeline(Config_params)\n",
    "    print(f\"Pipeline created for model: {model}, drop_categoric: {drop_categoric}\")\n",
    "    # Add your code here to train the pipeline or to print further details\n",
    "    \n",
    "    \n",
    "    pipeline.fit(X_train,y_train)\n",
    "    y_pred = pipeline.predict(X_validation)\n",
    "    \n",
    "    # check errors             \n",
    "    rms_log = np.sqrt(mean_squared_error(np.log(y_validation), np.log(y_pred)))        \n",
    "    print(f\"logRMS = {rms_log:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class keep_columns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns_to_keep):        \n",
    "        self.columns_to_keep = columns_to_keep\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[self.columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose=False\n",
    "general_pipeline = Pipeline([\n",
    "    ('drop_ID', drop_ID(verbose=Config_params['verbose'])),\n",
    "    ('remove_nas_col', remove_NAs_col(threshold=Config_params['nan_counts_threshold'], verbose=verbose)),\n",
    "    ('imput_nas_row', imput_NAs_row(verbose=verbose)),\n",
    "    ('handle_categoric', handle_categoric(categortic_Config_params,verbose=verbose))\n",
    "])\n",
    "y_new = y\n",
    "_X = general_pipeline.fit_transform(X, y_new)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(_X, y_new, test_size=0.2, random_state=42)\n",
    "verbose = Config_params['verbose']\n",
    "drop_categoric = \"TransformAll\"\n",
    "Config_params[\"categortic_Config_params\"] = {**categortic_Config_params, \"drop_categoric\": drop_categoric}\n",
    "Config_params['drop_categoric'] = \"TransformAll\"\n",
    "Config_params = Config_params\n",
    "model = CatBoostRegressor(random_state=42, verbose=int(Config_params['verbose']))\n",
    "cat_pipeline = Pipeline([\n",
    "    ('drop_ID', drop_ID(verbose=Config_params['verbose'])),\n",
    "    ('remove_nas_col', remove_NAs_col(threshold=Config_params['nan_counts_threshold'], verbose=verbose)),\n",
    "    ('imput_nas_row', imput_NAs_row(verbose=verbose)),\n",
    "    ('handle_categoric', handle_categoric(categortic_Config_params,verbose=verbose)),\n",
    "    ('reg', model)  # Use the dynamically selected model\n",
    "])\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('drop_ID', drop_ID(verbose=Config_params['verbose'])),\n",
    "    ('remove_nas_col', remove_NAs_col(threshold=Config_params['nan_counts_threshold'], verbose=verbose)),\n",
    "    ('imput_nas_row', imput_NAs_row(verbose=verbose)),\n",
    "    ('handle_categoric', handle_categoric(categortic_Config_params,verbose=verbose)),\n",
    "    ('reg', model)  # Use the dynamically selected model\n",
    "])\n",
    "model = KNeighborsRegressor(n_neighbors=5)\n",
    "kmeans_pipeline = Pipeline([\n",
    "    ('drop_ID', drop_ID(verbose=Config_params['verbose'])),\n",
    "    ('remove_nas_col', remove_NAs_col(threshold=Config_params['nan_counts_threshold'], verbose=verbose)),\n",
    "    ('imput_nas_row', imput_NAs_row(verbose=verbose)),\n",
    "    ('handle_categoric', handle_categoric(categortic_Config_params,verbose=verbose)),\n",
    "    ('keep_columns', keep_columns(['Neighborhood'])),\n",
    "    ('reg', model)  # Use the dynamically selected model\n",
    "])\n",
    "\n",
    "model = LinearRegression()\n",
    "lreg_pipeline = Pipeline([\n",
    "    ('drop_ID', drop_ID(verbose=Config_params['verbose'])),\n",
    "    ('remove_nas_col', remove_NAs_col(threshold=Config_params['nan_counts_threshold'], verbose=verbose)),\n",
    "    ('imput_nas_row', imput_NAs_row(verbose=verbose)),\n",
    "    ('handle_categoric', handle_categoric(categortic_Config_params,verbose=verbose)),\n",
    "    #('keep_columns', keep_columns(['Neighborhood'])),\n",
    "    ('reg', model)  # Use the dynamically selected model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostRegressor(random_state=42, verbose=int(Config_params['verbose']))\n",
    "cat_pipeline = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('reg', model)  # Use the dynamically selected model\n",
    "])\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('scaler', RobustScaler()),\n",
    "    ('reg', model)  # Use the dynamically selected model\n",
    "])\n",
    "model = KNeighborsRegressor(n_neighbors=50)\n",
    "kmeans_pipeline = Pipeline([\n",
    "    ('keep_columns', keep_columns(['Neighborhood'])),\n",
    "    ('reg', model)  # Use the dynamically selected model\n",
    "])\n",
    "\n",
    "model = LinearRegression()\n",
    "lreg_pipeline = Pipeline([\n",
    "    ('reg', model)  # Use the dynamically selected model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\n",
    "alphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n",
    "e_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\n",
    "e_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n",
    "kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "ridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\n",
    "lasso = make_pipeline(RobustScaler(), LassoCV(max_iter=10000000, alphas=alphas2, random_state=42, cv=kfolds))\n",
    "elasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=10000000, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))                                \n",
    "svr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003,))\n",
    "\n",
    "\n",
    "gbr = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05, max_depth=4, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =42) \n",
    "                                   \n",
    "\n",
    "lightgbm = LGBMRegressor(objective='regression', \n",
    "                                       num_leaves=4,\n",
    "                                       learning_rate=0.01, \n",
    "                                       n_estimators=5000,\n",
    "                                       max_bin=200, \n",
    "                                       bagging_fraction=0.75,\n",
    "                                       bagging_freq=5, \n",
    "                                       bagging_seed=7,\n",
    "                                       feature_fraction=0.2,\n",
    "                                       feature_fraction_seed=7,\n",
    "                                       verbose=-1,\n",
    "                                       )\n",
    "                                       \n",
    "\n",
    "xgboost = XGBRegressor(learning_rate=0.01,n_estimators=3460,\n",
    "                                     max_depth=3, min_child_weight=0,\n",
    "                                     gamma=0, subsample=0.7,\n",
    "                                     colsample_bytree=0.7,\n",
    "                                     objective='reg:linear', nthread=-1,\n",
    "                                     scale_pos_weight=1, seed=27,\n",
    "                                     reg_alpha=0.00006)\n",
    "\n",
    "# stack\n",
    "stack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet,\n",
    "                                            gbr, xgboost, lightgbm),\n",
    "                                meta_regressor=xgboost,\n",
    "                                use_features_in_secondary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def blend_models_predict(X):\n",
    "#     return ((0.1 * elastic_model_full_data.predict(X)) + \\\n",
    "#             (0.05 * lasso_model_full_data.predict(X)) + \\\n",
    "#             (0.1 * ridge_model_full_data.predict(X)) + \\\n",
    "#             (0.1 * svr_model_full_data.predict(X)) + \\\n",
    "#             (0.1 * gbr_model_full_data.predict(X)) + \\\n",
    "#             (0.15 * xgb_model_full_data.predict(X)) + \\\n",
    "#             (0.1 * lgb_model_full_data.predict(X)) + \\\n",
    "#             (0.3 * stack_gen_model.predict(np.array(X))))\n",
    "    \n",
    "# print('START Fit')\n",
    "\n",
    "# print('stack_gen')\n",
    "# stack_gen_model = stack_gen.fit(X_train, y_train)\n",
    "\n",
    "# print('elasticnet')\n",
    "# elastic_model_full_data = elasticnet.fit(X_train, y_train)\n",
    "\n",
    "# print('Lasso')\n",
    "# lasso_model_full_data = lasso.fit(X_train, y_train)\n",
    "\n",
    "# print('Ridge')\n",
    "# ridge_model_full_data = ridge.fit(X_train, y_train)\n",
    "\n",
    "# print('Svr')\n",
    "# svr_model_full_data = svr.fit(X_train, y_train)\n",
    "\n",
    "# print('GradientBoosting')\n",
    "# gbr_model_full_data = gbr.fit(X_train, y_train)\n",
    "\n",
    "# print('xgboost')\n",
    "# xgb_model_full_data = xgboost.fit(X_train, y_train)\n",
    "\n",
    "# print('lightgbm')\n",
    "# lgb_model_full_data = lightgbm.fit(X_train, y_train)\n",
    "\n",
    "# predict_func = blend_models_predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = [\n",
    "    ('cat', cat_pipeline),\n",
    "    ('xgb', xgb_pipeline),\n",
    "    ('gbr', gbr),\n",
    "    ('lreg', lreg_pipeline),\n",
    "    ('lasso', lasso),\n",
    "    ('elasticnet', elasticnet),\n",
    "    ('svr', svr),\n",
    "    ('lightgbm', lightgbm),\n",
    "    ('xgboost', xgboost),\n",
    "    ('rf', RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "]\n",
    "X_combined = pd.concat([X_train, X_validation], axis=0)\n",
    "y_combined = pd.concat([y_train, y_validation], axis=0)\n",
    "base_models = (cat_pipeline, xgb_pipeline, gbr,lreg_pipeline,lasso,elasticnet,svr, lightgbm,xgboost,RandomForestRegressor(n_estimators=100, random_state=42))\n",
    "base_models = (cat_pipeline, gbr, lightgbm, xgboost)\n",
    "meta_learner = LinearRegression()\n",
    "stacking_regressor = StackingRegressor(estimators=base_models, final_estimator=meta_learner)\n",
    "# stacking_regressor = StackingCVRegressor(regressors=base_models,\n",
    "#                                 meta_regressor=meta_learner,\n",
    "#                                 use_features_in_secondary=True)\n",
    "# stacking_regressor.fit(X_train, y_train)       \n",
    "# predict_func = stacking_regressor.predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models:\n",
    "    rms_log = cv_rmse(model[1], X_combined, y_combined)\n",
    "    print(f\"model : {model[0]} mean score : {rms_log.mean()} std : {rms_log.std()}\")\n",
    "# rms_log = cv_rmse(cat_pipeline, X_combined, y_combined)\n",
    "# print(\"cat boost regressor: {:.4f} ({:.4f})\\n\".format(rms_log.mean(), rms_log.std()))\n",
    "# rms_log = cv_rmse(xgb_pipeline, X_combined, y_combined)\n",
    "# print(\"xgb_pipeline regressor: {:.4f} ({:.4f})\\n\".format(rms_log.mean(), rms_log.std()))\n",
    "# rms_log = cv_rmse(gbr, X_combined, y_combined)\n",
    "# print(\"gbr regressor: {:.4f} ({:.4f})\\n\".format(rms_log.mean(), rms_log.std()))\n",
    "# rms_log = cv_rmse(lreg_pipeline, X_combined, y_combined)\n",
    "# print(\"lreg_pipeline regressor: {:.4f} ({:.4f})\\n\".format(rms_log.mean(), rms_log.std()))\n",
    "# rms_log = cv_rmse(lasso, X_combined, y_combined)\n",
    "# print(\"lasso regressor: {:.4f} ({:.4f})\\n\".format(rms_log.mean(), rms_log.std()))\n",
    "# rms_log = cv_rmse(elasticnet, X_combined, y_combined)\n",
    "# print(\"elasticnet: {:.4f} ({:.4f})\\n\".format(rms_log.mean(), rms_log.std()))\n",
    "# rms_log = cv_rmse(svr, X_combined, y_combined)\n",
    "# print(\"svr: {:.4f} ({:.4f})\\n\".format(rms_log.mean(), rms_log.std()))\n",
    "# rms_log = cv_rmse(svr, X_combined, y_combined)\n",
    "# print(\"svr: {:.4f} ({:.4f})\\n\".format(rms_log.mean(), rms_log.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rms_log = cv_rmse(stacking_regressor, X_combined, y_combined)\n",
    "print(\"stacking regressor: {:.4f} ({:.4f})\\n\".format(rms_log.mean(), rms_log.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rms_log = cv_rmse(stacking_regressor, X_combined, y_combined)\n",
    "print(\"stacking cv regressor: {:.4f} ({:.4f})\\n\".format(rms_log.mean(), rms_log.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = predict_func(X_validation)\n",
    "y_test = y_validation\n",
    "rms_log = np.sqrt(mean_squared_error(y_test, y_pred)) \n",
    "print(f\"logRMS = {rms_log:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv_results = stacking_regressor.cv_results_\n",
    "cv_results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "Finally predict on the competition test data using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit again now on full data_set to improve results\n",
    "if 1:\n",
    "    y_pred = CatKeepAllModel(X,y,testset_df)\n",
    "    #pipeline.fit(X,y)\n",
    "    # predinct on test\n",
    "    #y_pred = pipeline.predict(testset_df)\n",
    "if 0:\n",
    "    y_pred = CombModel(X,y,testset_df)\n",
    "    \n",
    "y_pred = np.exp(y_pred)\n",
    " \n",
    "output = pd.DataFrame({'Id': id_file,\n",
    "                       'SalePrice': y_pred.squeeze()})\n",
    "# create CSV\n",
    "output.to_csv('./project/submission.csv', index=False)\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "# Fit the scaler to the pivot table data and transform the data\n",
    "# standardized_data = pd.DataFrame(scaler.fit_transform(numeric_features_df), columns=numeric_features_df.columns, index=numeric_features_df.index)\n",
    "# lof_clf = LocalOutlierFactor()\n",
    "# lof_clf.fit_predict(standardized_data)\n",
    "# lof_scores = lof_clf.negative_outlier_factor_\n",
    "# lof_scores = pd.DataFrame({'score': lof_scores}, index=numeric_features_df.index)\n",
    "# combined_df = pd.concat([lof_scores, numeric_features_df], axis=1).sort_values('score')\n",
    "# combined_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
