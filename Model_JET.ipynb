{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "Learn to use visualization techniques to study:\n",
    "1. missing data\n",
    "2. distributions\n",
    "3. correlation heatmaps\n",
    "4. pairplots,\n",
    "5. t-SNE\n",
    "\n",
    "pre-proc:\n",
    "1. use catboost for categoric data\n",
    "\n",
    "model: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5v5mm4amQRrm",
    "papermill": {
     "duration": 0.010092,
     "end_time": "2023-03-07T06:21:39.774967",
     "exception": false,
     "start_time": "2023-03-07T06:21:39.764875",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# JET House Prices Prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# instruction\n",
    "## Part 1:\n",
    "[Model submission is done through Kaggle]\n",
    "Part 1 - EDA\n",
    "1. Which 3 features have the highest number of missing values\n",
    "2. How the price behave over the years?\n",
    "3. Plot the the feature distribution using histograms\n",
    "4. Compute and order the features by their correlation with label\n",
    "5. Add more EDA that will help you understand the data and support your modeling decisions\n",
    "\n",
    "Part 2 - baseline\n",
    "1. Train the simplest baseline model possible\n",
    "2. submit your baseline results and share the results\n",
    "\n",
    "## Part 2:\n",
    "[Model submission is done through Kaggle]\n",
    "Your solution:  go wild and build the best performing model!Which model you choose and why (relate to relevant EDA)?\n",
    "Choose a validation creation process, why you choose it? what is the baseline performance?\n",
    "Which smart tricks you used to boost your model performance?\n",
    "Describe potential generalization issues (e.g. overfit/underfit)? How can you handle these?\n",
    "submit your model and improve your rank in the leaderboard!\n",
    "\n",
    "Deliverables\n",
    "A final submission and score of you team in the leaderboard\n",
    "Working and easy to follow notebook with the 3 parts completed and that produce your best submission.\n",
    "Slides explaining your project: \n",
    "1. EDA\n",
    "2. decision made\n",
    "3. feature analysis\n",
    "4. validation\n",
    "5. modeling\n",
    "6. etc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVOXAyXl3-fA",
    "papermill": {
     "duration": 0.008317,
     "end_time": "2023-03-07T06:21:39.809564",
     "exception": false,
     "start_time": "2023-03-07T06:21:39.801247",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports and loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IGmyjJJatzBZ",
    "papermill": {
     "duration": 8.300496,
     "end_time": "2023-03-07T06:21:48.118668",
     "exception": false,
     "start_time": "2023-03-07T06:21:39.818172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#import tensorflow_decision_forests as tfdf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.stats import norm\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostRegressor\n",
    "import warnings\n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Comment this if the data visualisations doesn't work on your side\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# general functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printt(df,Name=None,verbose=True):    \n",
    "    if verbose==True:\n",
    "        print('----------------------')\n",
    "        if Name!=None:\n",
    "            print(Name)\n",
    "        if isinstance(df, dict):\n",
    "            for key, value in df.items():\n",
    "                print(f\"{key}: {value}\")\n",
    "        else:\n",
    "            print(df)\n",
    "               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_perd(y, y_pred,verbose=True,addline = True):\n",
    "    if verbose==True:\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.scatter(y, y_pred, color='blue')\n",
    "        plt.title('True vs Predicted Values')\n",
    "        plt.xlabel('True Values')\n",
    "        plt.ylabel('Predicted Values')\n",
    "        if addline: plt.plot([min(y), max(y)], [min(y), max(y)], color='red') # Line for perfect predictions\n",
    "        plt.grid(True)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_file_path = \"project/test.csv\"\n",
    "train_file_path = \"project/train.csv\"\n",
    "testset_df = pd.read_csv(test_file_path)\n",
    "dataset_df = pd.read_csv(train_file_path)\n",
    "print(\"Full test set shape is {}\".format(testset_df.shape))\n",
    "print(\"Full train dataset shape is {}\".format(dataset_df.shape))\n",
    "id_file = testset_df['Id']\n",
    "# Assuming 'dataset_df' is your DataFrame\n",
    "y = dataset_df['SalePrice']\n",
    "X = dataset_df.drop('SalePrice', axis=1)\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTnx8h9i416m",
    "papermill": {
     "duration": 0.008651,
     "end_time": "2023-03-07T06:21:48.263024",
     "exception": false,
     "start_time": "2023-03-07T06:21:48.254373",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The data is composed of 81 columns and 1460 entries. We can see all 81 dimensions of our dataset by printing out the first 3 entries using the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## explain_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_feature(feature_name, file_path=\"project/data_description.txt\"):\n",
    "    feature_name_lower = feature_name.lower()\n",
    "    with open(file_path, \"r\") as file:\n",
    "        lines = file.readlines()\n",
    "        collecting = False\n",
    "        for line in lines:\n",
    "            if feature_name_lower in line.lower():\n",
    "                collecting = True\n",
    "            elif collecting and \":\" in line and not line.lower().startswith(feature_name_lower):\n",
    "                # If we're collecting and encounter a line with a colon that doesn't start with the feature name,\n",
    "                # it's likely the start of another feature's description.\n",
    "                break\n",
    "            \n",
    "            if collecting:\n",
    "                print(line.strip())\n",
    "\n",
    "# Example usage\n",
    "explain_feature(\"MSSubClass\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## categoric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_order_categoric_to_numeric(X,y=None,verbose=False):\n",
    "# Assuming X is your DataFrame\n",
    " if 0:\n",
    "    category_sets = [\n",
    "        set(['Ex', 'Gd', 'TA', 'Fa', 'Po', 'NotExist']),\n",
    "        set(['Gd', 'Av', 'Mn', 'No', 'NotExist']),\n",
    "        set(['GLQ','ALQ','BLQ','Rec','LwQ','Unf','NotExist']),\n",
    "        set(['Fin','RFn','Unf','NotExist']),\n",
    "        set(['GdPrv','MnPrv','GdWo','MnWw','NotExist']),\n",
    "    ]\n",
    "    titles = ['quality_mapping', 'access_mapping', 'basement_mapping', 'garage_mapping', 'fence_mapping']\n",
    "    \n",
    "    # Mappings\n",
    "    quality_mapping = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NotExist': 0}\n",
    "    access_mapping = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NotExist': 0}\n",
    "    basement_mapping = {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NotExist': 0}\n",
    "    garage_mapping = {'Fin': 3, 'RFn': 2, 'Unf': 1, 'NA': 0}\n",
    "    fence_mapping = {'GdPrv': 4, 'MnPrv': 3, 'GdWo': 2, 'MnWw': 1, 'NotExist': 0}\n",
    "    \n",
    "    # Mapping from titles to actual mappings\n",
    "    mapping_dict = {\n",
    "        'quality_mapping': quality_mapping,\n",
    "        'access_mapping': access_mapping,\n",
    "        'basement_mapping': basement_mapping,\n",
    "        'garage_mapping': garage_mapping,\n",
    "        'fence_mapping': fence_mapping,\n",
    "    }\n",
    "    \n",
    "    # Select columns of object type\n",
    "    col_object = X.select_dtypes(include=['O']).columns.tolist()\n",
    "    \n",
    "    matching_categories = {}\n",
    "    for col in col_object:\n",
    "        vals = set(X[col].unique())\n",
    "        # Check which category set vals belongs to\n",
    "        for index, category_set in enumerate(category_sets):\n",
    "            if vals.issubset(category_set):  # Check if all elements of vals are in the category_set\n",
    "                matching_categories[col] = titles[index]                \n",
    "                break  # Exit the loop if a matching set is found\n",
    "\n",
    "    # Apply the matched mappings to the columns\n",
    "    X_ = X.copy()\n",
    "    for col, title in matching_categories.items():\n",
    "        if title in mapping_dict:\n",
    "            X_[col] = X[col].map(mapping_dict[title])\n",
    "\n",
    "    if verbose==True:\n",
    "        printt(len(matching_categories),'matching_categories')\n",
    "        print(list(matching_categories.keys()))\n",
    "        for col, title in matching_categories.items():\n",
    "            if y is not None and not y.empty:\n",
    "                categoric_analysis(X[[col]],y,True)\n",
    "            print(f\"{col}: {title} : {mapping_dict[title]}\")\n",
    "    \n",
    "    return X_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoric_analysis(X,y,verbose=False):\n",
    "    #col_object = X.select_dtypes(include=['O']).columns.tolist()\n",
    "    cols = X.columns.tolist()\n",
    "    #cols = X.columns.tolist()\n",
    "    printt(len(cols),'number of categorical',verbose=verbose)\n",
    "    printt(cols,'categorical List',verbose=verbose)\n",
    "    i=0\n",
    "    for col in cols:\n",
    "        i = i+1\n",
    "        vals = X[col].unique()        \n",
    "        cols = [col for _ in range(len(vals))]  # This will create a list with 'title' repeated\n",
    "        #print('\\n'.join(str(val) for val in vals))\n",
    "        ymean  = [np.mean(y[X[col] == val])  for val in vals]\n",
    "        ystd   = [np.std(y[X[col] == val])   for val in vals]\n",
    "        ratios = [np.mean(X[col] == val)*100         for val in vals]\n",
    "        numbers = [np.sum(X[col] == val)             for val in vals]\n",
    "\n",
    "        df = pd.DataFrame({'col': cols, 'val': vals, 'ymean': ymean, 'ystd': ystd,'number': numbers, 'ratio': ratios})\n",
    "        df.loc[pd.isna(vals), 'ratio'] = np.mean(X[col].isna())*100\n",
    "        df.loc[pd.isna(vals), 'number'] = np.sum(X[col].isna())\n",
    "        df.loc[pd.isna(vals), 'ymean'] = np.mean(y[X[col].isna()])\n",
    "        df.loc[pd.isna(vals), 'ystd'] = np.std(y[X[col].isna()])\n",
    "        df = df.sort_values(by='ymean', ascending=True).reset_index(drop=True)  # Use ascending=False for descending order\n",
    "        # This will apply the formatting and then convert the DataFrame to a string for printing\n",
    "        formatted_df_string = df.to_string(formatters={'ymean': \"{:.2f}\".format, 'ystd': \"{:.2f}\".format, 'ratio': \"{:.2f}\".format})\n",
    "        printt('',col,verbose=verbose)      \n",
    "        if verbose==True:        \n",
    "            print(i)\n",
    "            explain_feature(col)\n",
    "            print(formatted_df_string)        \n",
    "            plot_categoric(df,col)\n",
    "    return df\n",
    "\n",
    "def plot_categoric(df,col):\n",
    "        ymean = df['ymean'].values\n",
    "        ystd = df['ystd'].values\n",
    "        numbers = df['number'].values\n",
    "        vals = df['val'].values        \n",
    "            # Plotting\n",
    "        plt.figure(figsize=(2, 2))\n",
    "        colors = plt.cm.rainbow(np.linspace(0, 1, len(vals)))  # Generate distinct colors\n",
    "\n",
    "        for i, mean in enumerate(ymean):\n",
    "            # Generate x values\n",
    "            x = np.linspace(norm.ppf(0.01, loc=mean, scale=ystd[i]),\n",
    "                            norm.ppf(0.99, loc=mean, scale=ystd[i]), 100)\n",
    "            # Generate y values for Gaussian curve\n",
    "            y_gauss = norm.pdf(x, loc=mean, scale=ystd[i]) * numbers[i]  # Scale by 'num'\n",
    "            plt.plot(x, y_gauss, label=f'{vals[i]} (n={numbers[i]})', color=colors[i])\n",
    "\n",
    "        plt.title(f'{col}')\n",
    "        plt.legend()\n",
    "        plt.legend(loc='upper left', bbox_to_anchor=(1, 1))\n",
    "        plt.subplots_adjust(right=0.75)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "#categoric_analysis(X[['ExterQual']],y,True)\n",
    "#categoric_analysis(X[['Condition1']],y,True)\n",
    " \n",
    "manual_feature_importance = {\n",
    "    'not': ['Street','LandContour','Utilities','LotConfig','LandSlope','Condition2','RoofMatl','ExterCond','Heating','Functional','GarageQual','GarageCond','PavedDrive','PoolQC','Fence','MiscFeature'],\n",
    "    'low': ['Alley','Condition1','BsmtCond','BsmtFinType2','CentralAir','Electrical'],\n",
    "    'med': ['MSZoning','BldgType','HouseStyle','RoofStyle','Exterior1st','Exterior2nd','BsmtExposure','BsmtFinType1','SaleType','SaleCondition'],\n",
    "    'high': ['LotShape','Neighborhood','MasVnrType','ExterQual','Foundation','BsmtQual','HeatingQC','KitchenQual','FireplaceQu','GarageType','GarageFinish']\n",
    "}\n",
    "#example of use\n",
    "if 0:\n",
    "    categoric_analysis(X[manual_feature_importance['not']],np.log(y),True)\n",
    "if 0:\n",
    "    df = categoric_analysis(X[['Neighborhood']],np.log(y),True)\n",
    "if 1:\n",
    "    df = categoric_analysis(X[['MoSold','MSSubClass']],np.log(y),True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categoric_to_numeric_maping(df,MinRatioPerGroup = 10 ,verbose=False):\n",
    "    #df = pd.DataFrame({'col': cols, 'val': vals, 'ymean': ymean, 'ystd': ystd,'number': numbers, 'ratio': ratios})\n",
    "    MinRatioPerGroup = 100/np.floor(100/MinRatioPerGroup)\n",
    "    # diveide to groups\n",
    "    sum_ratio = 0\n",
    "    group = 0\n",
    "    for i in range(len(df)):\n",
    "        sum_ratio = sum_ratio + df.loc[i,'ratio']        \n",
    "        df.loc[i,'group'] = group\n",
    "        if sum_ratio>=MinRatioPerGroup:\n",
    "           sum_ratio = 0\n",
    "           group = group + 1        \n",
    "    # add last group to prev if it's too small\n",
    "    last_group_indexes = (df['group'] == group)\n",
    "    if np.sum(df.loc[last_group_indexes,'ratio'])<MinRatioPerGroup:\n",
    "        df.loc[last_group_indexes,'group'] = group-1\n",
    "\n",
    "    # add last group to prev if it's too small\n",
    "    if np.sum(df.loc[last_group_indexes,'ratio'])<MinRatioPerGroup:        \n",
    "        df.loc[last_group_indexes,'group'] = group-1\n",
    "\n",
    "    NGroups = int(df['group'].max()) + 1\n",
    "    # calc mean y per group\n",
    "    for g in range(NGroups):\n",
    "        group_indexes = (df['group'] == g)\n",
    "        ymean_group = np.sum(df.loc[group_indexes,'ymean']*df.loc[group_indexes,'number'])/np.sum(df.loc[group_indexes,'number'])\n",
    "        df.loc[group_indexes,'ymean_group'] = ymean_group\n",
    "        df.loc[group_indexes,'number_group'] = np.sum(df.loc[group_indexes,'number'])\n",
    "        df.loc[group_indexes,'ratio_group'] = np.sum(df.loc[group_indexes,'ratio'])\n",
    "    #print(df)\n",
    "    val_to_ymean_map = df.set_index('val')['ymean_group'].to_dict()\n",
    "    val_to_group_map = df.set_index('val')['group'].to_dict()\n",
    "    return val_to_ymean_map,val_to_group_map\n",
    "\n",
    "\n",
    "def categoric_to_numeric_fit(X,y,categortic_config_params,cols=None, JustCategoric=True,verbose=False):\n",
    "    MinRatioPerGroup = categortic_config_params['MinRatioPerGroup']\n",
    "    if JustCategoric==True:\n",
    "        exist_cols = X.select_dtypes(include=['O']).columns.tolist()\n",
    "    else:\n",
    "        exist_cols = X.columns.tolist()\n",
    "\n",
    "    if cols is None:\n",
    "        cols = exist_cols\n",
    "    else:\n",
    "        filtered_cols = [col for col in cols if col in exist_cols]\n",
    "        cols = filtered_cols\n",
    "    categoric_map = dict()\n",
    "    for col in cols:\n",
    "        df = categoric_analysis(X[[col]],y,False)\n",
    "        val_to_ymean_map,val_togroup_map = categoric_to_numeric_maping(df,MinRatioPerGroup,verbose=verbose)   \n",
    "        if categortic_config_params['replaceby']=='ymean':\n",
    "            categoric_map[col] = val_to_ymean_map\n",
    "        else:\n",
    "            categoric_map[col] = val_togroup_map  \n",
    "        printt(df,'df in categoric_to_numeric_fit',verbose=verbose)\n",
    "        printt(categoric_map,'categoric_map in categoric_to_numeric_fit',verbose=verbose)\n",
    "    return categoric_map\n",
    "def categoric_to_numeric_transform(X,categoric_map,verbose=False):\n",
    "    X_ = X.copy()\n",
    "    for col in categoric_map:\n",
    "        X_[col] = X_[col].map(categoric_map[col])        \n",
    "    return X_\n",
    "\n",
    "#ExampleOfuse\n",
    "if 1:\n",
    "    categortic_config_params1 = dict()\n",
    "    categortic_config_params1['MinRatioPerGroup'] = 15\n",
    "    categortic_config_params1['replaceby'] = 'ymean' #'ymean', 'group_nember'\n",
    "    cols = manual_feature_importance['high']\n",
    "    categoric_map = categoric_to_numeric_fit(X,y,categortic_config_params1,cols=cols,JustCategoric=True,verbose=False)\n",
    "    X_numeric = categoric_to_numeric_transform(X,categoric_map,verbose=False)\n",
    "    \n",
    "    col_to_map_num = ['MSSubClass']\n",
    "    categoric_map_num = categoric_to_numeric_fit(X_numeric,y,categortic_config_params1,cols=col_to_map_num,JustCategoric=False,verbose=False)\n",
    "    print(categoric_map_num)\n",
    "    X_numeric = categoric_to_numeric_transform(X_numeric,categoric_map_num,verbose=False)\n",
    "    \n",
    "    col_object1 = X.select_dtypes(include=['O']).columns.tolist()\n",
    "    col_object2 = X_numeric.select_dtypes(include=['O']).columns.tolist()\n",
    "    print(len(col_object1),len(col_object2))\n",
    "    printt(X_numeric['MSSubClass'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre proccessing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer to remove NAs\n",
    "class drop_ID(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, verbose=False):        \n",
    "        self.verbose = verbose\n",
    "    def fit(self, X, y=None):\n",
    "        printt('','In drop_ID fit',verbose=self.verbose)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        printt(X.shape,'drop_ID in X.shape',verbose=self.verbose)\n",
    "        # Ensure 'Id' column is removed safely\n",
    "        if 'Id' in X.columns:\n",
    "            return X.drop('Id', axis=1)\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imput_mean_mode(X,mean_mode):\n",
    "    # impute missing value by pre calc mean or mode :\n",
    "    col_numeric = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    col_object = X.select_dtypes(include=['O']).columns.tolist()\n",
    "    # save the mean and the mode for later imputation\n",
    "    combined_stats = pd.Series(dtype=object)\n",
    "    for col in X.columns:        \n",
    "        X[col].fillna(mean_mode[col], inplace=True)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_mode(X):\n",
    "    # find the mean of numrical columns and the mode of object columns\n",
    "    # return :\n",
    "    # combined_stats = mean amd mode in the same shape as orign,\n",
    "    # col_numeric,col_object =list of numeric and obkect coumln names\n",
    "    col_numeric = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    col_object = X.select_dtypes(include=['O']).columns.tolist()\n",
    "    printt(col_numeric,'col_numeric',False)\n",
    "    printt(col_object,'col_object',False)\n",
    "    # save the mean and the mode for later imputation\n",
    "    means = X[col_numeric].mean()\n",
    "    modes = X[col_object].mode().iloc[0]\n",
    "    # Step 3: Combine results, maintaining the original arrangement\n",
    "    combined_stats = pd.Series(dtype=object)\n",
    "    for col in X.columns:\n",
    "        if col in means:\n",
    "            combined_stats[col] = means[col]\n",
    "        elif col in modes:\n",
    "            combined_stats[col] = modes[col]\n",
    "    return combined_stats,col_numeric,col_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bad_col(X,threshold=None,verbose=False):\n",
    "    # drop col with na in amount more than threshold\n",
    "    if threshold is not None:\n",
    "        thresh = len(X)*threshold//1        \n",
    "        na_counts    = X.isna().sum()\n",
    "        columns_to_keep_ = na_counts[na_counts < thresh].index\n",
    "        columns_to_remove_ = na_counts[na_counts >= thresh].index\n",
    "    else:\n",
    "        columns_to_keep_ = X.columns\n",
    "        columns_to_remove_ = pd.Index([])        \n",
    "\n",
    "    if verbose==True:\n",
    "        nan_counts  = X.isna().sum().sort_values(ascending=False).head(20)/len(X)*100\n",
    "        print(f\"features with the highest number of missing values in %\")\n",
    "        print(f\"{nan_counts}%\")\n",
    "        printt(len(columns_to_keep_),'len(columns_to_keep_)',verbose=verbose)\n",
    "        printt(len(columns_to_remove_),'len(columns_to_remove_)',verbose=verbose)\n",
    "        if len(columns_to_remove_)>0:\n",
    "            printt(columns_to_remove_,'columns_to_remove_',verbose=verbose)           \n",
    "\n",
    "    return columns_to_keep_,columns_to_remove_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_row_nan_counts(X,verbose=False):\n",
    "    # print number of Rows with missing values\n",
    "    row_nan_counts = X.isna().sum(axis=1) \n",
    "    top_row_nan_counts = row_nan_counts.sort_values(ascending=False)\n",
    "    if verbose==True:\n",
    "        print(\"number of Rows with missing values:\")\n",
    "        print(top_row_nan_counts.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_missingVal_groups(X,verbose=False):\n",
    "    row_nan_counts = X.isna().sum(axis=1) \n",
    "    rows_with_missing_values = X[row_nan_counts >0]\n",
    "    unique_missing_column_groups = set()\n",
    "    # Iterate through rows and identify unique groups of missing columns\n",
    "    for _, row in rows_with_missing_values.iterrows():\n",
    "        # Extract groups of 5 columns\n",
    "        groups = tuple(row.index[row.isna()])\n",
    "        # Add the unique group to the set\n",
    "        unique_missing_column_groups.add(groups)\n",
    "    \n",
    "    # Sort the unique groups by their length\n",
    "    sorted_unique_missing_column_groups = sorted(unique_missing_column_groups, key=lambda x: len(x))\n",
    "    # Print the unique groups of 5 missing columns\n",
    "    if verbose==True:\n",
    "        print('\\nmissing values groups:')\n",
    "        for i, group in enumerate(sorted_unique_missing_column_groups, start=1):\n",
    "            print(f\"Group {i} of {len(group)} missing columns: {group}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class remove_NAs_col(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=None,verbose=False):        \n",
    "        self.threshold = threshold  # Minimum non-NA values required to keep a column\n",
    "        self.verbose = verbose\n",
    "    def fit(self, X, y=None):\n",
    "        printt('','In RemoveNAs Col fit',verbose=self.verbose)\n",
    "        # If threshold is set, identify columns to keep based on the threshold\n",
    "        self.columns_to_keep_,self.columns_to_remove_ = find_bad_col(X,self.threshold,self.verbose)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        printt('','In RemoveNAs Col transform',verbose=self.verbose)\n",
    "        # Drop columns not meeting the threshold requirement\n",
    "        printt(X.shape,'X.shape',verbose=self.verbose)\n",
    "        X_transformed = X.loc[:, self.columns_to_keep_]\n",
    "        printt(X_transformed.shape,'X_transformed.shape',verbose=self.verbose)        \n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class imput_NAs_row(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, verbose=False):        \n",
    "        self.column_means_ = None\n",
    "        self.verbose = verbose\n",
    "        self.col_numeric = None\n",
    "        self.col_object = None\n",
    "    def fit(self, X, y=None):\n",
    "        printt('','In imput_NAs_row fit',verbose=self.verbose)\n",
    "        # Calculate mean values for each column, ignoring NA's\n",
    "        printt(X.shape,'imput_NAs_row in X.shape',verbose=self.verbose)\n",
    "        # save the mean and the mode for later imputation\n",
    "        self.mean_mode,self.col_numeric,self.col_object = calc_mean_mode(X)\n",
    "        print_row_nan_counts(X,verbose=self.verbose)\n",
    "        print_missingVal_groups(X,verbose=self.verbose)        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        printt('','In imput_NAs_row transform',verbose=self.verbose)\n",
    "        printt(X.shape,'X.shape',verbose=self.verbose)\n",
    "        print_row_nan_counts(X,verbose=self.verbose)\n",
    "\n",
    "        printt('','filing Garage and Bsmt',verbose=self.verbose)\n",
    "        # Replace NaN values in the specified columns for rows where 'TotalBsmtSF' is equal to 0 with 'NotExist'\n",
    "        basement_list = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\n",
    "        Garage_list = ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']\n",
    "        X.loc[X['TotalBsmtSF'] == 0, basement_list] = X.loc[X['TotalBsmtSF'] == 0, basement_list].fillna('NotExist')\n",
    "        X.loc[X['GarageArea'] == 0, Garage_list] = X.loc[X['GarageArea'] == 0, Garage_list].fillna('NotExist')\n",
    "        print_row_nan_counts(X,verbose=self.verbose)\n",
    "        # special imputation\n",
    "        printt('','special imputation',verbose=self.verbose)\n",
    "        X['GarageYrBlt'].fillna(X['YearBuilt'], inplace=True)        \n",
    "        X['BsmtFinType2'].fillna('NotExist', inplace=True)\n",
    "        X['BsmtExposure'].fillna('NotExist', inplace=True)\n",
    "        print_row_nan_counts(X,verbose=self.verbose)\n",
    "\n",
    "        printt('','imput_mean_mode',verbose=self.verbose)\n",
    "        X = imput_mean_mode(X,self.mean_mode)\n",
    "        print_row_nan_counts(X,verbose=self.verbose)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## handle categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_qual_categoric(X,y=None,verbose=False):\n",
    "# Assuming X is your DataFrame\n",
    "\n",
    "    category_sets = [\n",
    "        set(['Ex', 'Gd', 'TA', 'Fa', 'Po', 'NotExist']),\n",
    "        set(['Gd', 'Av', 'Mn', 'No', 'NotExist']),\n",
    "        set(['GLQ','ALQ','BLQ','Rec','LwQ','Unf','NotExist']),\n",
    "        set(['Fin','RFn','Unf','NotExist']),\n",
    "        set(['GdPrv','MnPrv','GdWo','MnWw','NotExist']),\n",
    "    ]\n",
    "    titles = ['quality_mapping', 'access_mapping', 'basement_mapping', 'garage_mapping', 'fence_mapping']\n",
    "    \n",
    "    # Mappings\n",
    "    quality_mapping = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NotExist': 0}\n",
    "    access_mapping = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NotExist': 0}\n",
    "    basement_mapping = {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NotExist': 0}\n",
    "    garage_mapping = {'Fin': 3, 'RFn': 2, 'Unf': 1, 'NA': 0}\n",
    "    fence_mapping = {'GdPrv': 4, 'MnPrv': 3, 'GdWo': 2, 'MnWw': 1, 'NotExist': 0}\n",
    "    \n",
    "    # Mapping from titles to actual mappings\n",
    "    mapping_dict = {\n",
    "        'quality_mapping': quality_mapping,\n",
    "        'access_mapping': access_mapping,\n",
    "        'basement_mapping': basement_mapping,\n",
    "        'garage_mapping': garage_mapping,\n",
    "        'fence_mapping': fence_mapping,\n",
    "    }\n",
    "    \n",
    "    # Select columns of object type\n",
    "    col_object = X.select_dtypes(include=['O']).columns.tolist()\n",
    "    \n",
    "    matching_categories = {}\n",
    "    for col in col_object:\n",
    "        vals = set(X[col].unique())\n",
    "        # Check which category set vals belongs to\n",
    "        for index, category_set in enumerate(category_sets):\n",
    "            if vals.issubset(category_set):  # Check if all elements of vals are in the category_set\n",
    "                matching_categories[col] = titles[index]                \n",
    "                break  # Exit the loop if a matching set is found\n",
    "\n",
    "    # Apply the matched mappings to the columns\n",
    "    X_ = X.copy()\n",
    "    for col, title in matching_categories.items():\n",
    "        if title in mapping_dict:\n",
    "            X_[col] = X[col].map(mapping_dict[title])\n",
    "\n",
    "    if verbose==True:\n",
    "        printt(len(matching_categories),'matching_categories')\n",
    "        print(list(matching_categories.keys()))\n",
    "        for col, title in matching_categories.items():\n",
    "            if y is not None and not y.empty:\n",
    "                categoric_analysis(X[[col]],y,True)\n",
    "            print(f\"{col}: {title} : {mapping_dict[title]}\")\n",
    "    \n",
    "    return X_\n",
    "\n",
    "#replace_qual_categoric()\n",
    "def print_features(X,y,col_list=None): \n",
    "    if col_list is None:\n",
    "        col_list = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual', 'GarageFinish', 'GarageQual', 'GarageCond']\n",
    "    columns_to_keep_,columns_to_remove_ = find_bad_col(X,.15)\n",
    "    Xnew = X.loc[:, columns_to_keep_]\n",
    "    print(Xnew.shape)\n",
    "    imp = imput_NAs_row()\n",
    "    imp.fit(Xnew)\n",
    "    Xnew = imp.transform(Xnew)\n",
    "    print(Xnew.shape)\n",
    "    X_ = replace_qual_categoric(Xnew[col_list],y,verbose=True)\n",
    "    Xnew.shape,X_.shape\n",
    "\n",
    "if 0:\n",
    "    print_features(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class handle_categoric(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,categortic_config_params,verbose=False,col_to_map=None):        \n",
    "        self.col_numeric = None\n",
    "        self.verbose = verbose\n",
    "        self.col_object = None\n",
    "        self.col_to_map = col_to_map\n",
    "        self.col_to_map_num = ['MoSold','MSSubClass']\n",
    "        self.categoric_map_num = None\n",
    "        self.categortic_config_params = categortic_config_params\n",
    "    def fit(self, X, y=None):\n",
    "        printt('','In categoric fit',verbose=self.verbose)\n",
    "        self.col_numeric = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "        self.col_object = X.select_dtypes(include=['O']).columns.tolist()\n",
    "        #col_to_map = manual_feature_importance['high']\n",
    "        col_to_map = self.col_object\n",
    "        self.categoric_map  = categoric_to_numeric_fit(X,y,self.categortic_config_params,cols=col_to_map,JustCategoric=True,verbose=self.verbose)\n",
    "        self.categoric_map_num = categoric_to_numeric_fit(X,y,self.categortic_config_params,cols=self.col_to_map_num,JustCategoric=False,verbose=self.verbose)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        print(f\"drop_categoric: {self.categortic_config_params['drop_categoric']}\")\n",
    "        # for now remove all categoric TODO: improve this\n",
    "        printt('','In categoric transform',verbose=self.verbose)\n",
    "        printt(X.shape,'X.shape',verbose=self.verbose)\n",
    "\n",
    "        if self.categortic_config_params['drop_categoric']=='KeepJustOrdered' :\n",
    "            X = replace_qual_categoric(X,verbose=self.verbose)\n",
    "            col_numeric = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "            left_objects = len(X.select_dtypes(include=['O']).columns.tolist())\n",
    "            printt('',f'number of num feature befor {len(self.col_numeric)}, added {len(col_numeric) - len(self.col_numeric)}, left_objects: {left_objects}',verbose=self.verbose)\n",
    "            X = X.loc[:, col_numeric]\n",
    "\n",
    "        if self.categortic_config_params['drop_categoric'] == 'TransformAll':       \n",
    "            printt('','I am in Transform Categoric',verbose=self.verbose)\n",
    "            X = categoric_to_numeric_transform(X,self.categoric_map,verbose=self.verbose)\n",
    "            X = categoric_to_numeric_transform(X,self.categoric_map_num,verbose=self.verbose)\n",
    "            #display(X)\n",
    "            col_numeric = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "            X = X.loc[:, col_numeric]            \n",
    "            \n",
    "        if self.categortic_config_params['drop_categoric'] =='DropAll':\n",
    "            col_numeric = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "            X = X.loc[:, col_numeric]\n",
    "\n",
    "        #printt(len(X.select_dtypes(include=['O']).columns.tolist()),'length Object')\n",
    "        #print(X.select_dtypes(include=['O']).columns.tolist())\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## terget_manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terget_manipulation(y,take_log_target=0,verbose=False):      \n",
    "        if take_log_target==0:                                    \n",
    "            if verbose==True:\n",
    "                print('no log on target')\n",
    "            return y\n",
    "        if take_log_target==1:                        \n",
    "            if verbose==True:\n",
    "                print('take log on target')\n",
    "            return np.log(y)\n",
    "        if take_log_target==-1:                        \n",
    "            if verbose==True:\n",
    "                print('un-do log on target')\n",
    "            return np.exp(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categortic_config_params = dict()\n",
    "categortic_config_params['MinRatioPerGroup'] = 10\n",
    "categortic_config_params['replaceby'] = 'ymean' #'ymean', 'group_nember'\n",
    "#categortic_config_params['categoric_to_replace'] ='just_high' # 'All','from_low_to_high','from_med_to_high','just_high'\n",
    "categortic_config_params['drop_categoric'] = 'TransformAll' #'KeepAll','KeepJustOrdered','TransformAll','DropAll', KeepAll is the best option\n",
    "\n",
    "Config_params = dict()\n",
    "Config_params['nan_counts_threshold'] = .15\n",
    "Config_params['filter_out_HighNA'] = 1\n",
    "Config_params[\"model\"]='CatBoost' #'DecisionTree','CatBoost', CatBoost is the best option\n",
    "Config_params[\"take_log_target\"] = 1 # 0,1  . 1 is the best option\n",
    "Config_params[\"verbose\"]=False # False,True\n",
    "Config_params[\"categortic_config_params\"] = categortic_config_params\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and return a pipeline based on the configuration\n",
    "def create_pipeline(config_params):\n",
    "    verbose = Config_params[\"verbose\"]\n",
    "    categortic_config_params = config_params[\"categortic_config_params\"]\n",
    "    printt(Config_params['model'])\n",
    "    # Dynamically select the model based on config_params\n",
    "    printt(Config_params['model'],'Model',verbose=verbose)\n",
    "    if config_params['model'] == 'LinearRegression':\n",
    "        model = LinearRegression()\n",
    "    if config_params['model'] == 'DecisionTree':\n",
    "        model = DecisionTreeRegressor(random_state=42)\n",
    "    elif config_params['model'] == 'CatBoost':\n",
    "        #\n",
    "        if categortic_config_params['drop_categoric'] == 'KeepAll':\n",
    "            categorical_columns = X.select_dtypes(include=['O']).columns.tolist()\n",
    "            categorical_columns = ['MSZoning', 'Street', 'LotShape', 'LandContour',\n",
    "                               'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood',\n",
    "                               'Condition1', 'Condition2', 'BldgType', 'HouseStyle',\n",
    "                               'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd',\n",
    "                               'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
    "                               'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n",
    "                               'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n",
    "                               'Functional', 'GarageType', 'GarageFinish', 'GarageQual',\n",
    "                               'GarageCond', 'PavedDrive', 'SaleType', 'SaleCondition']\n",
    "            model = CatBoostRegressor(random_seed=42, silent=True, cat_features = categorical_columns)\n",
    "        else:\n",
    "            model = CatBoostRegressor(random_state=42, verbose=int(verbose))  # Adjust verbosity for CatBoost\n",
    "        #printt(len(categorical_columns),'length categorical_columns')\n",
    "        #print(categorical_columns)\n",
    "        \n",
    "    pipeline = Pipeline([\n",
    "        ('drop_ID', drop_ID(verbose=verbose)),\n",
    "        ('remove_nas_col', remove_NAs_col(threshold=config_params['nan_counts_threshold'], verbose=verbose)),\n",
    "        ('imput_nas_row', imput_NAs_row(verbose=verbose)),\n",
    "        ('handle_categoric', handle_categoric(categortic_config_params,verbose=verbose)),\n",
    "        ('robust_scaler', RobustScaler()),\n",
    "        ('reg', model)  # Use the dynamically selected model\n",
    "    ])\n",
    "    \n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = create_pipeline(Config_params)\n",
    "XX = pipeline.named_steps['drop_ID'].fit_transform(X_train)\n",
    "XX = pipeline.named_steps['remove_nas_col'].fit_transform(XX)\n",
    "XX = pipeline.named_steps['imput_nas_row'].fit_transform(XX)\n",
    "XX = pipeline.named_steps['handle_categoric'].fit_transform(XX,y=np.log(y_train))\n",
    "XX = pipeline.named_steps['robust_scaler'].fit_transform(XX)\n",
    "#XX.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate over all combinations\n",
    "models = ['CatBoost'] #['DecisionTree', 'CatBoost']\n",
    "drop_categoric_options = ['TransformAll'] #'KeepAll','KeepJustOrdered','DropAll','TransformAll'\n",
    "\n",
    "i=0\n",
    "pipelines = {}\n",
    "for model in models:\n",
    "    for drop_categoric in drop_categoric_options:\n",
    "            Config_params[\"model\"] = model\n",
    "            Config_params[\"drop_categoric\"] = drop_categoric\n",
    "            Config_params[\"categortic_config_params\"] = {**categortic_config_params, \"drop_categoric\": drop_categoric}\n",
    "            pipeline = create_pipeline(Config_params)\n",
    "            print(f\"Pipeline created for model: {model}, drop_categoric: {drop_categoric}\")\n",
    "            # Add your code here to train the pipeline or to print further details\n",
    "    \n",
    "            new_y = terget_manipulation(y_train,take_log_target=Config_params['take_log_target'])\n",
    "    \n",
    "            print('Start fit')\n",
    "            pipeline.fit(X_train,new_y)\n",
    "            print('Start predict')\n",
    "            y_pred = pipeline.predict(X_validation)\n",
    "    \n",
    "            y_pred = terget_manipulation(y_pred,take_log_target=-1*Config_params['take_log_target'],verbose=Config_params[\"verbose\"])\n",
    "            # check errors \n",
    "            #mask = (np.log(y_validation) > 11) & (np.log(y_validation) < 13)\n",
    "            mask = np.log(y_validation) > 0\n",
    "            rms_log = np.sqrt(mean_squared_error(np.log(y_validation[mask]), np.log(y_pred[mask])))        \n",
    "            print(f\"logRMS = {rms_log:.5f}\")\n",
    "            pipelines[i] = pipeline\n",
    "            i=i+1     \n",
    "            plot_perd(np.log(y_validation), np.log(y_pred),True)\n",
    "\n",
    "\n",
    "if 0: print('\\n'.join(f\"{col}: {title}\" for col, title in Config_params.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#aaaa\n",
    "#rms_log = np.sqrt(mean_squared_error(np.log(y_validation, np.log(y_pred))))        \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelines[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post Proccessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Post_Proccessing(pipeline):\n",
    "    model = pipeline.named_steps['reg']\n",
    "    \n",
    "    # Step 3: Get Feature Importances\n",
    "    feature_importances = model.get_feature_importance()\n",
    "    feature_names = model.feature_names_\n",
    "    sorted_indices = np.argsort(feature_importances)[::-1]\n",
    "    \n",
    "    # Limit to first 8 features\n",
    "    top_indices = sorted_indices[:8]\n",
    "    top_importances = feature_importances[top_indices]\n",
    "    top_names = np.array(feature_names)[top_indices]\n",
    "\n",
    "    printt(top_names,'top_names')\n",
    "    #print_features(X,y,top_names)\n",
    "    # Plot\n",
    "    plt.figure(figsize=(3, 3))\n",
    "    plt.title(\"Top 8 Feature Importances\")\n",
    "    plt.barh(range(len(top_importances)), top_importances, align=\"center\")\n",
    "    plt.yticks(range(len(top_importances)), top_names)\n",
    "    plt.gca().invert_yaxis()  # Invert y-axis to have the feature with the highest importance on top\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def Post_Proccessing_shap(pipeline):\n",
    "    model = pipeline.named_steps['reg']\n",
    "\n",
    "    # Calculate SHAP values\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_train)\n",
    "    \n",
    "    # Summarize the effects of all the features\n",
    "    shap.summary_plot(shap_values, X_train, plot_type=\"bar\")\n",
    "\n",
    "Post_Proccessing(pipelines[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_perd(np.log(y_validation),np.log(y_validation)-np.log(y_pred),True,addline=False)\n",
    "plot_perd(np.log(y_validation),np.log(y_pred),True,addline=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ??"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Config_params[\"model\"] = model\n",
    "# Config_params[\"drop_categoric\"] = drop_categoric\n",
    "drop_categoric = \"TransformAll\"\n",
    "Config_params[\"categortic_config_params\"] = {**categortic_config_params, \"drop_categoric\": drop_categoric}\n",
    "pipeline = create_pipeline(Config_params)\n",
    "print(f\"Pipeline created for model: {model}, drop_categoric: {drop_categoric}\")\n",
    "# Add your code here to train the pipeline or to print further details\n",
    "\n",
    "new_y = terget_manipulation(y_train,take_log_target=Config_params['take_log_target'])\n",
    "\n",
    "pipeline.fit(X_train,new_y)\n",
    "y_pred = pipeline.predict(X_validation)\n",
    "\n",
    "y_pred = terget_manipulation(y_pred,take_log_target=-1*Config_params['take_log_target'],verbose=Config_params[\"verbose\"])\n",
    "# check errors             \n",
    "rms_log = np.sqrt(mean_squared_error(np.log(y_validation), np.log(y_pred)))        \n",
    "print(f\"logRMS = {rms_log:.5f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class keep_columns(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, columns_to_keep):        \n",
    "        self.columns_to_keep = columns_to_keep\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        return X[self.columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose=False\n",
    "general_pipeline = Pipeline([\n",
    "    ('drop_ID', drop_ID(verbose=Config_params['verbose'])),\n",
    "    ('remove_nas_col', remove_NAs_col(threshold=Config_params['nan_counts_threshold'], verbose=verbose)),\n",
    "    ('imput_nas_row', imput_NAs_row(verbose=verbose)),\n",
    "    ('handle_categoric', handle_categoric(categortic_config_params,verbose=verbose))\n",
    "])\n",
    "y_new = terget_manipulation(y,take_log_target=Config_params['take_log_target'])\n",
    "_X = general_pipeline.fit_transform(X, y_new)\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(_X, y_new, test_size=0.2, random_state=42)\n",
    "verbose = Config_params['verbose']\n",
    "drop_categoric = \"TransformAll\"\n",
    "Config_params[\"categortic_config_params\"] = {**categortic_config_params, \"drop_categoric\": drop_categoric}\n",
    "Config_params['drop_categoric'] = \"TransformAll\"\n",
    "config_params = Config_params\n",
    "model = CatBoostRegressor(random_state=42, verbose=int(Config_params['verbose']))\n",
    "cat_pipeline = Pipeline([\n",
    "    ('drop_ID', drop_ID(verbose=Config_params['verbose'])),\n",
    "    ('remove_nas_col', remove_NAs_col(threshold=Config_params['nan_counts_threshold'], verbose=verbose)),\n",
    "    ('imput_nas_row', imput_NAs_row(verbose=verbose)),\n",
    "    ('handle_categoric', handle_categoric(categortic_config_params,verbose=verbose)),\n",
    "    ('reg', model)  # Use the dynamically selected model\n",
    "])\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('drop_ID', drop_ID(verbose=Config_params['verbose'])),\n",
    "    ('remove_nas_col', remove_NAs_col(threshold=config_params['nan_counts_threshold'], verbose=verbose)),\n",
    "    ('imput_nas_row', imput_NAs_row(verbose=verbose)),\n",
    "    ('handle_categoric', handle_categoric(categortic_config_params,verbose=verbose)),\n",
    "    ('reg', model)  # Use the dynamically selected model\n",
    "])\n",
    "model = KNeighborsRegressor(n_neighbors=5)\n",
    "kmeans_pipeline = Pipeline([\n",
    "    ('drop_ID', drop_ID(verbose=Config_params['verbose'])),\n",
    "    ('remove_nas_col', remove_NAs_col(threshold=config_params['nan_counts_threshold'], verbose=verbose)),\n",
    "    ('imput_nas_row', imput_NAs_row(verbose=verbose)),\n",
    "    ('handle_categoric', handle_categoric(categortic_config_params,verbose=verbose)),\n",
    "    ('keep_columns', keep_columns(['Neighborhood'])),\n",
    "    ('reg', model)  # Use the dynamically selected model\n",
    "])\n",
    "\n",
    "model = LinearRegression()\n",
    "lreg_pipeline = Pipeline([\n",
    "    ('drop_ID', drop_ID(verbose=Config_params['verbose'])),\n",
    "    ('remove_nas_col', remove_NAs_col(threshold=config_params['nan_counts_threshold'], verbose=verbose)),\n",
    "    ('imput_nas_row', imput_NAs_row(verbose=verbose)),\n",
    "    ('handle_categoric', handle_categoric(categortic_config_params,verbose=verbose)),\n",
    "    #('keep_columns', keep_columns(['Neighborhood'])),\n",
    "    ('reg', model)  # Use the dynamically selected model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CatBoostRegressor(random_state=42, verbose=int(Config_params['verbose']))\n",
    "cat_pipeline = Pipeline([\n",
    "    ('reg', model)  # Use the dynamically selected model\n",
    "])\n",
    "model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, random_state=42)\n",
    "xgb_pipeline = Pipeline([\n",
    "    ('reg', model)  # Use the dynamically selected model\n",
    "])\n",
    "model = KNeighborsRegressor(n_neighbors=5)\n",
    "kmeans_pipeline = Pipeline([\n",
    "    ('keep_columns', keep_columns(['Neighborhood'])),\n",
    "    ('reg', model)  # Use the dynamically selected model\n",
    "])\n",
    "\n",
    "model = LinearRegression()\n",
    "lreg_pipeline = Pipeline([\n",
    "    ('reg', model)  # Use the dynamically selected model\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_models = [\n",
    "    ('cat', cat_pipeline),\n",
    "    ('xgb', xgb_pipeline)\n",
    "    #('lreg', lreg_pipeline),\n",
    "    #('kmn', kmeans_pipeline)\n",
    "]\n",
    "meta_learner = LinearRegression()\n",
    "stacking_regressor = StackingRegressor(estimators=base_models, final_estimator=meta_learner)\n",
    "stacking_regressor.fit(X_train, y_train)\n",
    "y_pred = stacking_regressor.predict(X_validation)\n",
    "y_pred = terget_manipulation(y_pred,take_log_target=-1*Config_params['take_log_target'],verbose=Config_params[\"verbose\"])\n",
    "# check errors     \n",
    "y_validation = terget_manipulation(y_validation,take_log_target=-1*Config_params['take_log_target'],verbose=Config_params[\"verbose\"])\n",
    "rms_log = np.sqrt(mean_squared_error(np.log(y_validation), np.log(y_pred)))        \n",
    "print(f\"logRMS = {rms_log:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "Finally predict on the competition test data using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit again now on full data_set to improve results\n",
    "y_new = terget_manipulation(y,take_log_target=Config_params['take_log_target'])\n",
    "pipeline.fit(X,y_new)\n",
    "# predinct on test\n",
    "y_pred = pipeline.predict(testset_df)\n",
    "y_pred = terget_manipulation(y_pred,take_log_target=-1*Config_params['take_log_target'])\n",
    " \n",
    "output = pd.DataFrame({'Id': id_file,\n",
    "                       'SalePrice': y_pred.squeeze()})\n",
    "# create CSV\n",
    "output.to_csv('./project/submission.csv', index=False)\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
