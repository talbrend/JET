{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "Learn to use visualization techniques to study:\n",
    "1. missing data\n",
    "2. distributions\n",
    "3. correlation heatmaps\n",
    "4. pairplots,\n",
    "5. t-SNE\n",
    "\n",
    "pre-proc:\n",
    "1. use catboost for categoric data\n",
    "\n",
    "model: \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5v5mm4amQRrm",
    "papermill": {
     "duration": 0.010092,
     "end_time": "2023-03-07T06:21:39.774967",
     "exception": false,
     "start_time": "2023-03-07T06:21:39.764875",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# JET House Prices Prediction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# instruction\n",
    "## Part 1:\n",
    "[Model submission is done through Kaggle]\n",
    "Part 1 - EDA\n",
    "1. Which 3 features have the highest number of missing values\n",
    "2. How the price behave over the years?\n",
    "3. Plot the the feature distribution using histograms\n",
    "4. Compute and order the features by their correlation with label\n",
    "5. Add more EDA that will help you understand the data and support your modeling decisions\n",
    "\n",
    "Part 2 - baseline\n",
    "1. Train the simplest baseline model possible\n",
    "2. submit your baseline results and share the results\n",
    "\n",
    "## Part 2:\n",
    "[Model submission is done through Kaggle]\n",
    "Your solution:  go wild and build the best performing model!Which model you choose and why (relate to relevant EDA)?\n",
    "Choose a validation creation process, why you choose it? what is the baseline performance?\n",
    "Which smart tricks you used to boost your model performance?\n",
    "Describe potential generalization issues (e.g. overfit/underfit)? How can you handle these?\n",
    "submit your model and improve your rank in the leaderboard!\n",
    "\n",
    "Deliverables\n",
    "A final submission and score of you team in the leaderboard\n",
    "Working and easy to follow notebook with the 3 parts completed and that produce your best submission.\n",
    "Slides explaining your project: \n",
    "1. EDA\n",
    "2. decision made\n",
    "3. feature analysis\n",
    "4. validation\n",
    "5. modeling\n",
    "6. etc\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FVOXAyXl3-fA",
    "papermill": {
     "duration": 0.008317,
     "end_time": "2023-03-07T06:21:39.809564",
     "exception": false,
     "start_time": "2023-03-07T06:21:39.801247",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Imports and loadings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "IGmyjJJatzBZ",
    "papermill": {
     "duration": 8.300496,
     "end_time": "2023-03-07T06:21:48.118668",
     "exception": false,
     "start_time": "2023-03-07T06:21:39.818172",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import tensorflow as tf\n",
    "#import tensorflow_decision_forests as tfdf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from catboost import CatBoostRegressor\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Comment this if the data visualisations doesn't work on your side\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# general functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printt(df,Name=None,verbose=True):    \n",
    "    if verbose==True:\n",
    "        print('----------------------')\n",
    "        if Name!=None:\n",
    "            print(Name)\n",
    "        print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_perd(y, y_pred,verbose=True):\n",
    "    if verbose==True:\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.scatter(y, y_pred, color='blue')\n",
    "        plt.title('True vs Predicted Values')\n",
    "        plt.xlabel('True Values')\n",
    "        plt.ylabel('Predicted Values')\n",
    "        plt.plot([min(y), max(y)], [min(y), max(y)], color='red') # Line for perfect predictions\n",
    "        plt.grid(True)\n",
    "        plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full test set shape is (1459, 80)\n",
      "Full train dataset shape is (1460, 81)\n"
     ]
    }
   ],
   "source": [
    "test_file_path = \"project/test.csv\"\n",
    "train_file_path = \"project/train.csv\"\n",
    "testset_df = pd.read_csv(test_file_path)\n",
    "dataset_df = pd.read_csv(train_file_path)\n",
    "print(\"Full test set shape is {}\".format(testset_df.shape))\n",
    "print(\"Full train dataset shape is {}\".format(dataset_df.shape))\n",
    "id_file = testset_df['Id']\n",
    "# Assuming 'dataset_df' is your DataFrame\n",
    "y = dataset_df['SalePrice']\n",
    "X = dataset_df.drop('SalePrice', axis=1)\n",
    "# Splitting the dataset into training and testing sets\n",
    "X_train, X_validation, y_train, y_validation = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mTnx8h9i416m",
    "papermill": {
     "duration": 0.008651,
     "end_time": "2023-03-07T06:21:48.263024",
     "exception": false,
     "start_time": "2023-03-07T06:21:48.254373",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "The data is composed of 81 columns and 1460 entries. We can see all 81 dimensions of our dataset by printing out the first 3 entries using the following code:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre proccessing functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## drop ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom transformer to remove NAs\n",
    "class drop_ID(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, verbose=False):        \n",
    "        self.verbose = verbose\n",
    "    def fit(self, X, y=None):\n",
    "        printt('','In drop_ID fit',verbose=self.verbose)\n",
    "        return self\n",
    "\n",
    "    def transform(self, X, y=None):\n",
    "        printt(X.shape,'drop_ID in X.shape',verbose=self.verbose)\n",
    "        # Ensure 'Id' column is removed safely\n",
    "        if 'Id' in X.columns:\n",
    "            return X.drop('Id', axis=1)\n",
    "        else:\n",
    "            return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NAs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def imput_mean_mode(X,mean_mode):\n",
    "    # impute missing value by pre calc mean or mode :\n",
    "    col_numeric = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    col_object = X.select_dtypes(include=['O']).columns.tolist()\n",
    "    # save the mean and the mode for later imputation\n",
    "    combined_stats = pd.Series(dtype=object)\n",
    "    for col in X.columns:        \n",
    "        X[col].fillna(mean_mode[col], inplace=True)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_mean_mode(X):\n",
    "    # find the mean of numrical columns and the mode of object columns\n",
    "    # return :\n",
    "    # combined_stats = mean amd mode in the same shape as orign,\n",
    "    # col_numeric,col_object =list of numeric and obkect coumln names\n",
    "    col_numeric = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "    col_object = X.select_dtypes(include=['O']).columns.tolist()\n",
    "    printt(col_numeric,'col_numeric',False)\n",
    "    printt(col_object,'col_object',False)\n",
    "    # save the mean and the mode for later imputation\n",
    "    means = X[col_numeric].mean()\n",
    "    modes = X[col_object].mode().iloc[0]\n",
    "    # Step 3: Combine results, maintaining the original arrangement\n",
    "    combined_stats = pd.Series(dtype=object)\n",
    "    for col in X.columns:\n",
    "        if col in means:\n",
    "            combined_stats[col] = means[col]\n",
    "        elif col in modes:\n",
    "            combined_stats[col] = modes[col]\n",
    "    return combined_stats,col_numeric,col_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_bad_col(X,threshold=None,verbose=False):\n",
    "    # drop col with na in amount more than threshold\n",
    "    if threshold is not None:\n",
    "        thresh = len(X)*threshold//1        \n",
    "        na_counts    = X.isna().sum()\n",
    "        columns_to_keep_ = na_counts[na_counts < thresh].index\n",
    "        columns_to_remove_ = na_counts[na_counts >= thresh].index\n",
    "    else:\n",
    "        columns_to_keep_ = X.columns\n",
    "        columns_to_remove_ = pd.Index([])        \n",
    "\n",
    "    if verbose==True:\n",
    "        nan_counts  = X.isna().sum().sort_values(ascending=False).head(20)/len(X)*100\n",
    "        print(f\"features with the highest number of missing values in %\")\n",
    "        print(f\"{nan_counts}%\")\n",
    "        printt(len(columns_to_keep_),'len(columns_to_keep_)',verbose=verbose)\n",
    "        printt(len(columns_to_remove_),'len(columns_to_remove_)',verbose=verbose)\n",
    "        if len(columns_to_remove_)>0:\n",
    "            printt(columns_to_remove_,'columns_to_remove_',verbose=verbose)           \n",
    "\n",
    "    return columns_to_keep_,columns_to_remove_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_row_nan_counts(X,verbose=False):\n",
    "    # print number of Rows with missing values\n",
    "    row_nan_counts = X.isna().sum(axis=1) \n",
    "    top_row_nan_counts = row_nan_counts.sort_values(ascending=False)\n",
    "    if verbose==True:\n",
    "        print(\"number of Rows with missing values:\")\n",
    "        print(top_row_nan_counts.value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_missingVal_groups(X,verbose=False):\n",
    "    row_nan_counts = X.isna().sum(axis=1) \n",
    "    rows_with_missing_values = X[row_nan_counts >0]\n",
    "    unique_missing_column_groups = set()\n",
    "    # Iterate through rows and identify unique groups of missing columns\n",
    "    for _, row in rows_with_missing_values.iterrows():\n",
    "        # Extract groups of 5 columns\n",
    "        groups = tuple(row.index[row.isna()])\n",
    "        # Add the unique group to the set\n",
    "        unique_missing_column_groups.add(groups)\n",
    "    \n",
    "    # Sort the unique groups by their length\n",
    "    sorted_unique_missing_column_groups = sorted(unique_missing_column_groups, key=lambda x: len(x))\n",
    "    # Print the unique groups of 5 missing columns\n",
    "    if verbose==True:\n",
    "        print('\\nmissing values groups:')\n",
    "        for i, group in enumerate(sorted_unique_missing_column_groups, start=1):\n",
    "            print(f\"Group {i} of {len(group)} missing columns: {group}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class remove_NAs_col(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, threshold=None,verbose=False):        \n",
    "        self.threshold = threshold  # Minimum non-NA values required to keep a column\n",
    "        self.verbose = verbose\n",
    "    def fit(self, X, y=None):\n",
    "        printt('','In RemoveNAs Col fit',verbose=self.verbose)\n",
    "        # If threshold is set, identify columns to keep based on the threshold\n",
    "        self.columns_to_keep_,self.columns_to_remove_ = find_bad_col(X,self.threshold,self.verbose)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        printt('','In RemoveNAs Col transform',verbose=self.verbose)\n",
    "        # Drop columns not meeting the threshold requirement\n",
    "        printt(X.shape,'X.shape',verbose=self.verbose)\n",
    "        X_transformed = X.loc[:, self.columns_to_keep_]\n",
    "        printt(X_transformed.shape,'X_transformed.shape',verbose=self.verbose)        \n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class imput_NAs_row(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, verbose=False):        \n",
    "        self.column_means_ = None\n",
    "        self.verbose = verbose\n",
    "        self.col_numeric = None\n",
    "        self.col_object = None\n",
    "    def fit(self, X, y=None):\n",
    "        printt('','In imput_NAs_row fit',verbose=self.verbose)\n",
    "        # Calculate mean values for each column, ignoring NA's\n",
    "        printt(X.shape,'imput_NAs_row in X.shape',verbose=self.verbose)\n",
    "        # save the mean and the mode for later imputation\n",
    "        self.mean_mode,self.col_numeric,self.col_object = calc_mean_mode(X)\n",
    "        print_row_nan_counts(X,verbose=self.verbose)\n",
    "        print_missingVal_groups(X,verbose=self.verbose)        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        printt('','In imput_NAs_row transform',verbose=self.verbose)\n",
    "        printt(X.shape,'X.shape',verbose=self.verbose)\n",
    "        print_row_nan_counts(X,verbose=self.verbose)\n",
    "\n",
    "        printt('','filing Garage and Bsmt',verbose=self.verbose)\n",
    "        # Replace NaN values in the specified columns for rows where 'TotalBsmtSF' is equal to 0 with 'NotExist'\n",
    "        basement_list = ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']\n",
    "        Garage_list = ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']\n",
    "        X.loc[X['TotalBsmtSF'] == 0, basement_list] = X.loc[X['TotalBsmtSF'] == 0, basement_list].fillna('NotExist')\n",
    "        X.loc[X['GarageArea'] == 0, Garage_list] = X.loc[X['GarageArea'] == 0, Garage_list].fillna('NotExist')\n",
    "        print_row_nan_counts(X,verbose=self.verbose)\n",
    "        # special imputation\n",
    "        printt('','special imputation',verbose=self.verbose)\n",
    "        X['GarageYrBlt'].fillna(X['YearBuilt'], inplace=True)        \n",
    "        X['BsmtFinType2'].fillna('NotExist', inplace=True)\n",
    "        X['BsmtExposure'].fillna('NotExist', inplace=True)\n",
    "        print_row_nan_counts(X,verbose=self.verbose)\n",
    "\n",
    "        printt('','imput_mean_mode',verbose=self.verbose)\n",
    "        X = imput_mean_mode(X,self.mean_mode)\n",
    "        print_row_nan_counts(X,verbose=self.verbose)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## handle categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------\n",
      "ExterQual\n",
      "\n",
      "\n",
      "         col val ymean ystd  number ratio\n",
      "0  ExterQual  Gd 12.31 0.29     488 33.42\n",
      "1  ExterQual  TA 11.84 0.29     906 62.05\n",
      "2  ExterQual  Ex 12.76 0.32      52  3.56\n",
      "3  ExterQual  Fa 11.30 0.39      14  0.96\n"
     ]
    }
   ],
   "source": [
    "def print_categoric(X,y,verbose=False):\n",
    "    col_object = X.select_dtypes(include=['O']).columns.tolist()\n",
    "    for col in col_object:\n",
    "        printt('',col)        \n",
    "        vals = X[col].unique()        \n",
    "        print()\n",
    "        cols = [col for _ in range(len(vals))]  # This will create a list with 'title' repeated\n",
    "        #print('\\n'.join(str(val) for val in vals))\n",
    "        if 0:\n",
    "            print(np.mean(np.log(y[X[col] == 'TA'])))\n",
    "            print(np.mean(np.log(y[X[col] == 'Gd'])))\n",
    "            print(np.mean(np.log(y[X[col] == 'Ex'])))\n",
    "            print(np.mean(np.log(y[X[col] == 'Fa'])))\n",
    "        ymean  = [np.mean(np.log(y[X[col] == val]))  for val in vals]\n",
    "        ystd   = [np.std(np.log(y[X[col] == val]))   for val in vals]\n",
    "        ratios = [np.mean(X[col] == val)*100         for val in vals]\n",
    "        numbers = [np.sum(X[col] == val)             for val in vals]\n",
    "\n",
    "        df = pd.DataFrame({'col': cols, 'val': vals, 'ymean': ymean, 'ystd': ystd,'number': numbers, 'ratio': ratios})\n",
    "        df.loc[pd.isna(vals), 'ratio'] = np.mean(X[col].isna())*100\n",
    "        df.loc[pd.isna(vals), 'number'] = np.sum(X[col].isna())\n",
    "        df.loc[pd.isna(vals), 'ymean'] = np.mean(np.log(y[X[col].isna()]))\n",
    "        df.loc[pd.isna(vals), 'ystd'] = np.std(np.log(y[X[col].isna()]))\n",
    "        \n",
    "        # This will apply the formatting and then convert the DataFrame to a string for printing\n",
    "        formatted_df_string = df.to_string(formatters={'ymean': \"{:.2f}\".format, 'ystd': \"{:.2f}\".format, 'ratio': \"{:.2f}\".format})\n",
    "        print(formatted_df_string)        \n",
    "\n",
    "print_categoric(X[['ExterQual']],y,True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_qual_categoric(X,y=None,verbose=False):\n",
    "# Assuming X is your DataFrame\n",
    "\n",
    "    category_sets = [\n",
    "        set(['Ex', 'Gd', 'TA', 'Fa', 'Po', 'NotExist']),\n",
    "        set(['Gd', 'Av', 'Mn', 'No', 'NotExist']),\n",
    "        set(['GLQ','ALQ','BLQ','Rec','LwQ','Unf','NotExist']),\n",
    "        set(['Fin','RFn','Unf','NotExist']),\n",
    "        set(['GdPrv','MnPrv','GdWo','MnWw','NotExist']),\n",
    "    ]\n",
    "    titles = ['quality_mapping', 'access_mapping', 'basement_mapping', 'garage_mapping', 'fence_mapping']\n",
    "    \n",
    "    # Mappings\n",
    "    quality_mapping = {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NotExist': 0}\n",
    "    access_mapping = {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NotExist': 0}\n",
    "    basement_mapping = {'GLQ': 6, 'ALQ': 5, 'BLQ': 4, 'Rec': 3, 'LwQ': 2, 'Unf': 1, 'NotExist': 0}\n",
    "    garage_mapping = {'Fin': 3, 'RFn': 2, 'Unf': 1, 'NA': 0}\n",
    "    fence_mapping = {'GdPrv': 4, 'MnPrv': 3, 'GdWo': 2, 'MnWw': 1, 'NotExist': 0}\n",
    "    \n",
    "    # Mapping from titles to actual mappings\n",
    "    mapping_dict = {\n",
    "        'quality_mapping': quality_mapping,\n",
    "        'access_mapping': access_mapping,\n",
    "        'basement_mapping': basement_mapping,\n",
    "        'garage_mapping': garage_mapping,\n",
    "        'fence_mapping': fence_mapping,\n",
    "    }\n",
    "    \n",
    "    # Select columns of object type\n",
    "    col_object = X.select_dtypes(include=['O']).columns.tolist()\n",
    "    \n",
    "    matching_categories = {}\n",
    "    for col in col_object:\n",
    "        vals = set(X[col].unique())\n",
    "        # Check which category set vals belongs to\n",
    "        for index, category_set in enumerate(category_sets):\n",
    "            if vals.issubset(category_set):  # Check if all elements of vals are in the category_set\n",
    "                matching_categories[col] = titles[index]                \n",
    "                break  # Exit the loop if a matching set is found\n",
    "\n",
    "    # Apply the matched mappings to the columns\n",
    "    X_ = X.copy()\n",
    "    for col, title in matching_categories.items():\n",
    "        if title in mapping_dict:\n",
    "            X_[col] = X[col].map(mapping_dict[title])\n",
    "\n",
    "    if verbose==True:\n",
    "        printt(len(matching_categories),'matching_categories')\n",
    "        print(list(matching_categories.keys()))\n",
    "        for col, title in matching_categories.items():\n",
    "            if y is not None and not y.empty:\n",
    "                print_categoric(X[[col]],y,True)\n",
    "            print(f\"{col}: {title} : {mapping_dict[title]}\")\n",
    "    \n",
    "    return X_\n",
    "\n",
    "#replace_qual_categoric()\n",
    "if 0:\n",
    "    order_col = ['ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC', 'KitchenQual', 'GarageFinish', 'GarageQual', 'GarageCond']\n",
    "    columns_to_keep_,columns_to_remove_ = find_bad_col(X,.15)\n",
    "    Xnew = X.loc[:, columns_to_keep_]\n",
    "    print(Xnew.shape)\n",
    "    imp = imput_NAs_row()\n",
    "    imp.fit(Xnew)\n",
    "    Xnew = imp.transform(Xnew)\n",
    "    print(Xnew.shape)\n",
    "    X_ = replace_qual_categoric(Xnew[order_col],y,verbose=True)\n",
    "    Xnew.shape,X_.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class handle_categoric(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,verbose=False,drop_categoric=False):        \n",
    "        self.col_numeric = None\n",
    "        self.verbose = verbose\n",
    "        self.col_object = None\n",
    "        self.drop_categoric = drop_categoric\n",
    "    def fit(self, X,y=None):\n",
    "        printt('','In categoric fit',verbose=self.verbose)\n",
    "        self.col_numeric = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "        self.col_object = X.select_dtypes(include=['O']).columns.tolist()\n",
    "\n",
    "        #print_categoric(X)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # for now remove all categoric TODO: improve this\n",
    "        printt('','In categoric transform',verbose=self.verbose)\n",
    "        printt(X.shape,'X.shape',verbose=self.verbose)\n",
    "\n",
    "        if self.drop_categoric=='KeepJustOrdered' :\n",
    "            X = replace_qual_categoric(X,verbose=self.verbose)\n",
    "            col_numeric = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "            left_objects = len(X.select_dtypes(include=['O']).columns.tolist())\n",
    "            printt('',f'number of num feature befor {len(self.col_numeric)}, added {len(col_numeric) - len(self.col_numeric)}, left_objects: {left_objects}',verbose=self.verbose)\n",
    "            X = X.loc[:, col_numeric]\n",
    "            \n",
    "        if self.drop_categoric=='DropAll':\n",
    "            col_numeric = X.select_dtypes(include=['float64', 'int64']).columns.tolist()\n",
    "            X = X.loc[:, col_numeric]\n",
    "\n",
    "        #printt(len(X.select_dtypes(include=['O']).columns.tolist()),'length Object')\n",
    "        #print(X.select_dtypes(include=['O']).columns.tolist())\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## terget_manipulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def terget_manipulation(y,take_log_target=0,verbose=False):      \n",
    "        if take_log_target==0:                                    \n",
    "            if verbose==True:\n",
    "                print('no log on target')\n",
    "            return y\n",
    "        if take_log_target==1:                        \n",
    "            if verbose==True:\n",
    "                print('take log on target')\n",
    "            return np.log(y)\n",
    "        if take_log_target==-1:                        \n",
    "            if verbose==True:\n",
    "                print('un-do log on target')\n",
    "            return np.exp(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "Config_params = dict()\n",
    "Config_params['nan_counts_threshold'] = .15\n",
    "Config_params['filter_out_HighNA'] = 1\n",
    "Config_params[\"model\"]='decisionTree' #'decisionTree','CatBoost'\n",
    "Config_params[\"drop_categoric\"] = 'KeepAll' #'KeepAll','KeepJustOrdered','DropAll'\n",
    "Config_params[\"take_log_target\"]=0 # 0,1\n",
    "Config_params[\"verbose\"]=False # False,True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create and return a pipeline based on the configuration\n",
    "def create_pipeline(config_params):\n",
    "    verbose = Config_params[\"verbose\"]\n",
    "\n",
    "    # Dynamically select the model based on config_params\n",
    "    printt(Config_params['model'],'Model',verbose=verbose)\n",
    "    if config_params['model'] == 'DecisionTree':\n",
    "        model = DecisionTreeRegressor(random_state=42)\n",
    "    elif config_params['model'] == 'CatBoost':\n",
    "        #\n",
    "        if config_params.drop_categoric == 'KeepAll':\n",
    "            categorical_columns = X.select_dtypes(include=['O']).columns.tolist()\n",
    "            categorical_columns = ['MSZoning', 'Street', 'LotShape', 'LandContour',\n",
    "                               'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood',\n",
    "                               'Condition1', 'Condition2', 'BldgType', 'HouseStyle',\n",
    "                               'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd',\n",
    "                               'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
    "                               'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n",
    "                               'Heating', 'HeatingQC', 'CentralAir', 'Electrical', 'KitchenQual',\n",
    "                               'Functional', 'GarageType', 'GarageFinish', 'GarageQual',\n",
    "                               'GarageCond', 'PavedDrive', 'SaleType', 'SaleCondition']\n",
    "            model = CatBoostRegressor(random_seed=42, silent=True, cat_features = categorical_columns)\n",
    "        else:\n",
    "            model = CatBoostRegressor(random_state=42, verbose=int(verbose))  # Adjust verbosity for CatBoost\n",
    "        #printt(len(categorical_columns),'length categorical_columns')\n",
    "        #print(categorical_columns)\n",
    "        \n",
    "\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('drop_ID', drop_ID(verbose=verbose)),\n",
    "        ('remove_nas_col', remove_NAs_col(threshold=config_params['nan_counts_threshold'], verbose=verbose)),\n",
    "        ('imput_nas_row', imput_NAs_row(verbose=verbose)),\n",
    "        ('handle_categoric', handle_categoric(drop_categoric=config_params['drop_categoric'], verbose=verbose)),\n",
    "        ('reg', model)  # Use the dynamically selected model\n",
    "    ])\n",
    "    \n",
    "    return pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'drop_categoric'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[115], line 9\u001b[0m\n\u001b[0;32m      7\u001b[0m Config_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\n\u001b[0;32m      8\u001b[0m Config_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdrop_categoric\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m drop_categoric\n\u001b[1;32m----> 9\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[43mConfig_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPipeline created for model: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, drop_categoric: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdrop_categoric\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Add your code here to train the pipeline or to print further details\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[113], line 11\u001b[0m, in \u001b[0;36mcreate_pipeline\u001b[1;34m(config_params)\u001b[0m\n\u001b[0;32m      8\u001b[0m     model \u001b[38;5;241m=\u001b[39m DecisionTreeRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m config_params[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCatBoost\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     10\u001b[0m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m---> 11\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mconfig_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop_categoric\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mKeepAll\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m     12\u001b[0m         categorical_columns \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mselect_dtypes(include\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mO\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     13\u001b[0m         categorical_columns \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMSZoning\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mStreet\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLotShape\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLandContour\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     14\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUtilities\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLotConfig\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLandSlope\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNeighborhood\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     15\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCondition1\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCondition2\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBldgType\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mHouseStyle\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     20\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFunctional\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGarageType\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGarageFinish\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGarageQual\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     21\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGarageCond\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPavedDrive\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSaleType\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSaleCondition\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'drop_categoric'"
     ]
    }
   ],
   "source": [
    "# Iterate over all combinations\n",
    "models = ['CatBoost'] #['DecisionTree', 'CatBoost']\n",
    "drop_categoric_options = ['DropAll','KeepJustOrdered','KeepAll'] #'KeepAll','KeepJustOrdered','DropAll'\n",
    "\n",
    "for model in models:\n",
    "    for drop_categoric in drop_categoric_options:\n",
    "        Config_params[\"model\"] = model\n",
    "        Config_params[\"drop_categoric\"] = drop_categoric\n",
    "        pipeline = create_pipeline(Config_params)\n",
    "        print(f\"Pipeline created for model: {model}, drop_categoric: {drop_categoric}\")\n",
    "        # Add your code here to train the pipeline or to print further details\n",
    "\n",
    "        new_y = terget_manipulation(y_train,take_log_target=Config_params['take_log_target'])\n",
    "\n",
    "        pipeline.fit(X_train,new_y)\n",
    "        y_pred = pipeline.predict(X_validation)\n",
    "\n",
    "        y_pred = terget_manipulation(y_pred,take_log_target=-1*Config_params['take_log_target'],verbose=Config_params[\"verbose\"])\n",
    "        # check errors\n",
    "        rms = np.sqrt(mean_squared_error(y_validation, y_pred))        \n",
    "        print(f\"rms = {rms:.2f}, logRMS = {np.log(rms):.2f}\")\n",
    "\n",
    "if 0: print('\\n'.join(f\"{col}: {title}\" for col, title in Config_params.items()))\n",
    "\n",
    "plot_perd(y_validation, y_pred,True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission\n",
    "Finally predict on the competition test data using the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1461</td>\n",
       "      <td>116804.917550</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1462</td>\n",
       "      <td>161706.005026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1463</td>\n",
       "      <td>188186.589387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1464</td>\n",
       "      <td>191363.116421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1465</td>\n",
       "      <td>186629.846975</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Id      SalePrice\n",
       "0  1461  116804.917550\n",
       "1  1462  161706.005026\n",
       "2  1463  188186.589387\n",
       "3  1464  191363.116421\n",
       "4  1465  186629.846975"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit again now on full data_set to improve results\n",
    "pipeline.fit(X,y)\n",
    "# predinct on test\n",
    "y_pred = pipeline.predict(testset_df)\n",
    "output = pd.DataFrame({'Id': id_file,\n",
    "                       'SalePrice': y_pred.squeeze()})\n",
    "# create CSV\n",
    "output.to_csv('./project/submission.csv', index=False)\n",
    "output.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
